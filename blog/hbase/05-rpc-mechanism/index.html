<!doctype html><html class=scroll-smooth lang=en><head><meta charset=utf-8><link href=/css/style.css rel=stylesheet><link href=/line-awesome/css/line-awesome.min.css rel=stylesheet><script defer src=/js/main.js></script><title>HBase 源码阅读：05. RPC 通信机制 | 码农的自留地</title><body class="bg-white dark:bg-slate-900 transition ease-in-out"><section><div class="sticky top-0 bg-slate-100 dark:bg-slate-800"><div class="container mx-auto px-auto xl:px-0 w-full xl:w-1/2 flex place-content-between py-16 xl:py-8 font-sans text-6xl xl:text-2xl text-slate-900 dark:text-slate-300"><div class=flex><a class="m-0 p-0 text-slate-900 hover:text-slate-700 dark:text-slate-300 dark:hover:text-slate-200" href=/blog/> /blog </a><a class="m-0 p-0 text-slate-900 hover:text-slate-700 dark:text-slate-300 dark:hover:text-slate-200" href=/blog/hbase/> /hbase </a></div><div class="flex gap-4"><div class="hidden cursor-pointer" id=back-to-top><i class="las la-level-up-alt"></i></div><a href=/><i class="las la-home"></i></a><div class=cursor-pointer id=darkmode-toggle><div class="hidden dark:inline"><i class="las la-sun"></i></div><div class="inline dark:hidden"><i class="las la-moon"></i></div></div></div></div></div><div class="container mx-auto w-full xl:w-1/2 mb-16"><div class="mt-4 font-serif text-slate-600 dark:text-slate-500 text-4xl xl:text-base">2026-01-12</div><h1 class="w-full xl:w-2/3 mt-4 mb-8 font-serif text-8xl xl:text-4xl text-slate-900 dark:text-slate-300">HBase 源码阅读：05. RPC 通信机制</h1><div class="mt-2 mb-6 flex flex-wrap gap-2"></div><div class="w-100 border-t mb-8 border-slate-300 dark:border-slate-700"></div><div class="prose dark:prose-invert prose-pre:rounded-none prose-headings:bg-amber-100 prose-headings:text-slate-800 dark:prose-headings:bg-indigo-900 prose-headings:font-normal dark:prose-headings:text-slate-300 prose-headings:p-2 prose-headings:w-max prose-headings:font-serif prose-2xl xl:prose-base"><p>HBase 的高性能离不开其定制的 RPC 框架。HBase 2.x 全面拥抱 Netty，实现了异步非阻塞的 IO 模型。<h2 id=1-jia-gou-gai-lan>1. 架构概览</h2><ul><li><strong>序列化</strong>: Google Protocol Buffers (Protobuf)。所有的请求和响应对象都由 <code>.proto</code> 文件定义。<li><strong>传输层</strong>: Netty。支持 NIO 和 Epoll。<li><strong>服务模型</strong>: Client -> Netty Channel -> RpcServer -> Scheduler -> RpcHandler -> Service Implementation。</ul><h2 id=2-server-duan-shi-xian-nettyrpcserver>2. Server 端实现 (<code>NettyRpcServer</code>)</h2><h3 id=2-1-listener-yu-reader>2.1 Listener 与 Reader</h3><ul><li><strong>Listener</strong>: 监听端口，接受 TCP 连接。<li><strong>Netty Handler</strong>: <code>NettyRpcServerPreambleHandler</code>, <code>NettyRpcServerRequestDecoder</code>。 <ul><li>负责处理 TCP 粘包拆包，解析 Protobuf 头部。</ul></ul><h3 id=2-2-scheduler-diao-du-qi>2.2 Scheduler (调度器)</h3><p>解析出来的 Call 对象会被扔给 Scheduler。常见的实现是 <code>SimpleRpcScheduler</code>。 Scheduler 内部维护了多个队列，将请求分发给不同的 Handler 线程池：<ul><li><strong>Priority Queue</strong>: 高优先级请求（如 Meta 表操作）。<li><strong>General Queue</strong>: 普通读写请求。<li><strong>Replication Queue</strong>: 复制请求。</ul><h3 id=2-3-rpchandler>2.3 RpcHandler</h3><p>工作线程，从队列中取出 Call，执行真正的业务逻辑（调用 <code>RSRpcServices</code> 的方法），然后将结果写回 Netty Channel。<h2 id=3-client-duan-shi-xian-nettyrpcclient>3. Client 端实现 (<code>NettyRpcClient</code>)</h2><p>客户端通过 <code>Connection</code> 复用 TCP 连接。<ul><li><strong>AsyncProcess</strong>: 客户端的批量提交逻辑。<li><strong>RpcChannel</strong>: 维护与 RegionServer 的长连接。</ul><h2 id=4-guan-jian-pei-zhi>4. 关键配置</h2><ul><li><code>hbase.ipc.server.read.threadpool.size</code>: 读线程数。<li><code>hbase.ipc.server.callqueue.handler.factor</code>: 队列与 Handler 的比例。<li><code>hbase.ipc.server.callqueue.read.ratio</code>: 读写队列的比例。</ul><hr><p><strong>Next</strong>: <a href=../06-zookeeper-coordination/>HBase 源码阅读：06. ZooKeeper 协调机制</a></div></div></section>