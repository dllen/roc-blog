<!doctype html><html class=scroll-smooth lang=en><head><meta charset=utf-8><link href=/css/style.css rel=stylesheet><link href=/line-awesome/css/line-awesome.min.css rel=stylesheet><script defer src=/js/main.js></script><title>HDFS 源码阅读：01. 源码编译与环境搭建 | 码农的自留地</title><body class="bg-white dark:bg-slate-900 transition ease-in-out"><section><div class="sticky top-0 bg-slate-100 dark:bg-slate-800"><div class="container mx-auto px-auto xl:px-0 w-full xl:w-1/2 flex place-content-between py-16 xl:py-8 font-sans text-6xl xl:text-2xl text-slate-900 dark:text-slate-300"><div class=flex><a class="m-0 p-0 text-slate-900 hover:text-slate-700 dark:text-slate-300 dark:hover:text-slate-200" href=/blog/> /blog </a><a class="m-0 p-0 text-slate-900 hover:text-slate-700 dark:text-slate-300 dark:hover:text-slate-200" href=/blog/hdfs/> /hdfs </a></div><div class="flex gap-4"><div class="hidden cursor-pointer" id=back-to-top><i class="las la-level-up-alt"></i></div><a href=/><i class="las la-home"></i></a><div class=cursor-pointer id=darkmode-toggle><div class="hidden dark:inline"><i class="las la-sun"></i></div><div class="inline dark:hidden"><i class="las la-moon"></i></div></div></div></div></div><div class="container mx-auto w-full xl:w-1/2 mb-16"><div class="mt-4 font-serif text-slate-600 dark:text-slate-500 text-4xl xl:text-base">2026-01-12</div><h1 class="w-full xl:w-2/3 mt-4 mb-8 font-serif text-8xl xl:text-4xl text-slate-900 dark:text-slate-300">HDFS 源码阅读：01. 源码编译与环境搭建</h1><div class="mt-2 mb-6 flex flex-wrap gap-2"></div><div class="w-100 border-t mb-8 border-slate-300 dark:border-slate-700"></div><div class="prose dark:prose-invert prose-pre:rounded-none prose-headings:bg-amber-100 prose-headings:text-slate-800 dark:prose-headings:bg-indigo-900 prose-headings:font-normal dark:prose-headings:text-slate-300 prose-headings:p-2 prose-headings:w-max prose-headings:font-serif prose-2xl xl:prose-base"><p>在开始深入代码之前，我们需要先把环境搭好。Hadoop 是一个庞大的 Java 项目，构建过程可能会遇到各种环境依赖问题。<h2 id=1-yuan-ma-xia-zai>1. 源码下载</h2><p>建议阅读 <strong>Hadoop 3.3.x</strong> 版本的代码，这是目前生产环境的主流版本。<pre class=language-bash data-lang=bash><code class=language-bash data-lang=bash>git clone https://github.com/apache/hadoop.git
cd hadoop
git checkout rel/release-3.3.6
</code></pre><h2 id=2-xiang-mu-jie-gou-gai-lan>2. 项目结构概览</h2><p>Hadoop 采用 Maven 多模块构建。对于 HDFS 阅读，我们主要关注 <code>hadoop-hdfs-project</code> 目录。<p>核心模块如下：<ul><li><strong>hadoop-common-project/hadoop-common</strong>: 基础库，包括 RPC 框架、配置系统、IO 工具类等。<li><strong>hadoop-hdfs-project/hadoop-hdfs</strong>: HDFS 的核心实现（NameNode, DataNode 等）。<li><strong>hadoop-hdfs-project/hadoop-hdfs-client</strong>: HDFS 客户端实现，为了解耦从 core 中拆分出来。<li><strong>hadoop-mapreduce-project</strong>: MapReduce 实现（本次暂不关注）。<li><strong>hadoop-yarn-project</strong>: YARN 实现（本次暂不关注）。</ul><h2 id=3-yuan-ma-bian-yi>3. 源码编译</h2><p>Hadoop 依赖 Protocol Buffers (protobuf) 2.5.0 (部分模块可能需要更新版本) 和 CMake (用于编译 native 库)。<h3 id=3-1-qian-zhi-yao-qiu-macos>3.1 前置要求 (macOS)</h3><pre class=language-bash data-lang=bash><code class=language-bash data-lang=bash>brew install maven cmake protobuf autoconf automake libtool
</code></pre><p><em>注意：Hadoop 3.3+ 可能需要 Protobuf 3.7.1+。请参考源码根目录下的 <code>BUILDING.txt</code>。</em><h3 id=3-2-bian-yi-ming-ling>3.2 编译命令</h3><p>如果我们只是为了阅读源码和在 IDEA 中跳转，<strong>不需要编译 Native 库</strong>，也不需要打包 tarball。<p>只需生成 Protobuf 对应的 Java 代码即可：<pre class=language-bash data-lang=bash><code class=language-bash data-lang=bash>mvn clean install -DskipTests -Dmaven.javadoc.skip=true -Pdist -Dtar
</code></pre><p>如果为了节省时间，可以只编译 hdfs 模块：<pre class=language-bash data-lang=bash><code class=language-bash data-lang=bash>cd hadoop-hdfs-project
mvn clean install -DskipTests -Dmaven.javadoc.skip=true
</code></pre><h2 id=4-dao-ru-intellij-idea>4. 导入 IntelliJ IDEA</h2><ol><li>打开 IDEA，选择 <code>Open</code>，选中 <code>hadoop</code> 根目录下的 <code>pom.xml</code>。<li>等待 Maven 依赖下载完成。<li><strong>关键步骤</strong>：确保生成的源码目录被识别。 <ul><li>Hadoop 使用 Protobuf 生成了很多代码，通常位于 <code>target/generated-sources/java</code> 下。<li>如果 IDEA 报红找不到类（如 <code>ClientNamenodeProtocolProtos</code>），需手动将 <code>target/generated-sources</code> 标记为 <code>Generated Sources Root</code>。</ul></ol><h2 id=5-diao-shi-huan-jing-pei-zhi>5. 调试环境配置</h2><p>最简单的调试方法是利用 Hadoop 的 <code>MiniDFSCluster</code>。这是一个在单进程中启动 NameNode and DataNodes 的测试工具，非常适合断点调试。<p>创建一个简单的 JUnit 测试用例：<pre class=language-java data-lang=java><code class=language-java data-lang=java>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hdfs.MiniDFSCluster;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import java.io.IOException;

public class HdfsDebugDemo {

    private MiniDFSCluster cluster;
    private FileSystem fs;
    private Configuration conf;

    @Before
    public void setUp() throws IOException {
        conf = new Configuration();
        // 启动一个 NameNode 和 3 个 DataNode
        cluster = new MiniDFSCluster.Builder(conf).numDataNodes(3).build();
        cluster.waitActive();
        fs = cluster.getFileSystem();
    }

    @Test
    public void testWriteAndRead() throws IOException {
        Path file = new Path("/test/hello.txt");
        
        // 在此处打断点，跟踪 create 流程
        fs.create(file).close();
        
        // 在此处打断点，跟踪 open 流程
        fs.open(file).close();
    }

    @After
    public void tearDown() {
        if (cluster != null) {
            cluster.shutdown();
        }
    }
}
</code></pre><p>运行这个 Test，你就可以在 <code>DFSOutputStream</code> 或 <code>NameNode</code> 的相关代码中停下来观察了。</div></div></section>