---
title: "Brex æç¤ºå·¥ç¨‹æŒ‡å—ï¼ˆåŒè¯­å¯¹ç…§ï¼‰"
date: "2025-11-13"
update_date: "2025-11-13"
description: "Brex's Prompt Engineering Guide ä¸“ä¸šåŒè¯­è¯‘ç¨¿ï¼Œä¿ç•™åŸç»“æ„ä¸ä»£ç "
tldr: "å®Œæ•´ä¿ç•™åŸæ–‡ç»“æ„ä¸æ ¼å¼ï¼Œé€æ®µè‹±ä¸­å¯¹ç…§ç¿»è¯‘ï¼Œæœ¯è¯­ç»Ÿä¸€"
---

åŸæ–‡å‡ºå¤„å£°æ˜

English: Brex's Prompt Engineering Guide â€” https://github.com/brexhq/prompt-engineering

ä¸­æ–‡ï¼šBrex æç¤ºå·¥ç¨‹æŒ‡å—ï¼ˆGitHub ä»“åº“ï¼‰â€” https://github.com/brexhq/prompt-engineering

ç¿»è¯‘è¯´æ˜

English: Translation follows a "one English paragraph + one Chinese translation" format. All code blocks and special formatting are preserved. Terminology conforms to industry standards (e.g., LLM, Chain-of-Thought, RLHF, context window).

ä¸­æ–‡ï¼šé‡‡ç”¨â€œä¸€æ®µè‹±æ–‡åŸæ–‡ + ä¸€æ®µä¸­æ–‡è¯‘æ–‡â€çš„å¯¹ç…§æ ¼å¼ï¼›å®Œæ•´ä¿ç•™ä»£ç å—ä¸ç‰¹æ®Šæ ¼å¼ï¼›æœ¯è¯­æŒ‰è¡Œä¸šæ ‡å‡†ç»Ÿä¸€ï¼ˆå¦‚ LLMã€æ€ç»´é“¾ã€RLHFã€ä¸Šä¸‹æ–‡çª—å£ï¼‰ã€‚

æ­£æ–‡ï¼ˆåŒè¯­å¯¹ç…§ï¼‰

# Brex's Prompt Engineering Guide

Brex's Prompt Engineering Guide

Brex æç¤ºå·¥ç¨‹æŒ‡å—

This guide was created by Brex for internal purposes. It's based on lessons learned from researching and creating Large Language Model (LLM) prompts for production use cases. It covers the history around LLMs as well as strategies, guidelines, and safety recommendations for working with and building programmatic systems on top of large language models, like OpenAI's GPT-4.

æœ¬æŒ‡å—ç”± Brex ä¸ºå†…éƒ¨ä½¿ç”¨è€Œç¼–å†™ï¼ŒåŸºäºåœ¨ç”Ÿäº§ç”¨ä¾‹ä¸­ç ”ç©¶ä¸æ„å»ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æç¤ºè¯çš„ç»éªŒæ€»ç»“ã€‚å†…å®¹æ¶µç›– LLM çš„å‘å±•è„‰ç»œï¼Œä»¥åŠåœ¨åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ OpenAI çš„ GPT-4ï¼‰ä¹‹ä¸Šæ„å»ºç¨‹åºåŒ–ç³»ç»Ÿæ—¶çš„ç­–ç•¥ã€å‡†åˆ™ä¸å®‰å…¨å»ºè®®ã€‚

The examples in this document were generated with a non-deterministic language model and the same examples may give you different results.

æœ¬æ–‡ç¤ºä¾‹ç”±éç¡®å®šæ€§è¯­è¨€æ¨¡å‹ç”Ÿæˆï¼Œç›¸åŒç¤ºä¾‹åœ¨ä¸åŒè¿è¡Œä¸­å¯èƒ½äº§ç”Ÿä¸åŒç»“æœã€‚

This is a living document. The state-of-the-art best practices and strategies around LLMs are evolving rapidly every day. Discussion and suggestions for improvements are encouraged.

æœ¬æŒ‡å—æ˜¯åŠ¨æ€æ›´æ–°çš„æ–‡æ¡£ã€‚å›´ç»• LLM çš„æœ€ä½³å®è·µä¸ç­–ç•¥æ¯å¤©éƒ½åœ¨å¿«é€Ÿæ¼”è¿›ï¼Œæ¬¢è¿è®¨è®ºä¸æ”¹è¿›å»ºè®®ã€‚

## Table of Contents

Table of Contents

ç›®å½•

What is a Large Language Model?

ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼Ÿ

A Brief, Incomplete, and Somewhat Incorrect History of Language Models

è¯­è¨€æ¨¡å‹ç®€å²ï¼ˆä¸å®Œæ•´ä¸”ç•¥æœ‰åå·®ï¼‰

Pre-2000â€™s

2000 å¹´å‰

Mid-2000â€™s

2000 å¹´ä»£ä¸­æœŸ

Early-2010â€™s

2010 å¹´ä»£æ—©æœŸ

Late-2010â€™s

2010 å¹´ä»£åæœŸ

2020â€™s

2020 å¹´ä»£

What is a prompt?

ä»€ä¹ˆæ˜¯æç¤ºè¯ï¼Ÿ

Hidden Prompts

éšè—æç¤ºè¯

Tokens

è¯å…ƒï¼ˆTokenï¼‰

Token Limits

è¯å…ƒé™åˆ¶

Prompt Hacking

æç¤ºè¯æ”»é˜²

Jailbreaks

è¶Šç‹±

Leaks

ä¿¡æ¯æ³„éœ²

Why do we need prompt engineering?

ä¸ºä½•éœ€è¦æç¤ºå·¥ç¨‹ï¼Ÿ

Give a Bot a Fish

æˆä¹‹ä»¥é±¼

Semantic Search

è¯­ä¹‰æ£€ç´¢

Teach a Bot to Fish

æˆä¹‹ä»¥æ¸”

Command Grammars

å‘½ä»¤è¯­æ³•ï¼ˆGrammarsï¼‰

ReAct

æ€è€ƒ-è¡ŒåŠ¨ï¼ˆReActï¼‰

GPT-4 vs GPT-3.5

GPT-4 ä¸ GPT-3.5 å¯¹æ¯”

Strategies

ç­–ç•¥

Embedding Data

åµŒå…¥æ•°æ®

Simple Lists

ç®€å•åˆ—è¡¨

Markdown Tables

Markdown è¡¨æ ¼

JSON

JSON

Freeform Text

è‡ªç”±æ–‡æœ¬

Nested Data

åµŒå¥—æ•°æ®

Citations

å¼•ç”¨

Programmatic Consumption

ç¨‹åºåŒ–æ¶ˆè´¹

Chain of Thought

æ€ç»´é“¾ï¼ˆChain of Thoughtï¼‰

Averaging

å¤šæ ·æœ¬å¹³å‡

Interpreting Code

ä»£ç è§£é‡Š

Delimiters

åˆ†éš”ç¬¦

Fine Tuning

å¾®è°ƒï¼ˆFine-tuningï¼‰

Downsides

é™åˆ¶ä¸ä»£ä»·

Additional Resources

æ‰©å±•èµ„æº

## What is a Large Language Model (LLM)?

What is a Large Language Model (LLM)?

ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Ÿ

A large language model is a prediction engine that takes a sequence of words and tries to predict the most likely sequence to come after that sequence1. It does this by assigning a probability to likely next sequences and then samples from those to choose one2. The process repeats until some stopping criteria is met.

å¤§è¯­è¨€æ¨¡å‹æ˜¯ä¸€ç§é¢„æµ‹å¼•æ“ï¼Œå®ƒæ¥æ”¶ä¸€ä¸ªè¯åºåˆ—å¹¶å°è¯•é¢„æµ‹æœ€å¯èƒ½çš„åç»­è¯åºåˆ—ã€‚æ¨¡å‹ä¸ºå¯èƒ½çš„ä¸‹ä¸€ä¸ªåºåˆ—èµ‹äºˆæ¦‚ç‡ï¼Œç„¶åè¿›è¡Œé‡‡æ ·é€‰æ‹©å…¶ä¸­ä¸€ä¸ªï¼Œé‡å¤è¯¥è¿‡ç¨‹ç›´åˆ°æ»¡è¶³åœæ­¢æ¡ä»¶ã€‚

Large language models learn these probabilities by training on large corpuses of text. A consequence of this is that the models will cater to some use cases better than others (e.g. if itâ€™s trained on GitHub data, itâ€™ll understand the probabilities of sequences in source code really well). Another consequence is that the model may generate statements that seem plausible, but are actually just random without being grounded in reality.

LLM é€šè¿‡åœ¨æµ·é‡æ–‡æœ¬è¯­æ–™ä¸Šè®­ç»ƒæ¥å­¦ä¹ è¿™äº›æ¦‚ç‡ã€‚è¿™å¯¼è‡´æ¨¡å‹åœ¨éƒ¨åˆ†åœºæ™¯è¡¨ç°æ›´å¥½ï¼ˆå¦‚è®­ç»ƒå«æœ‰ GitHub æ•°æ®çš„æ¨¡å‹æ›´æ“…é•¿æºç åºåˆ—ï¼‰ï¼Œä¹Ÿå¯èƒ½ç”Ÿæˆâ€œçœ‹ä¼¼åˆç†ä½†æ— äº‹å®ä¾æ®â€çš„å†…å®¹ï¼ˆå¹»è§‰ï¼‰ã€‚

As language models become more accurate at predicting sequences, many surprising abilities emerge.

éšç€è¯­è¨€æ¨¡å‹çš„åºåˆ—é¢„æµ‹èƒ½åŠ›æå‡ï¼Œè®¸å¤šä»¤äººæƒŠè®¶çš„èƒ½åŠ›ä¼šæ¶Œç°ã€‚

## A Brief, Incomplete, and Somewhat Incorrect History of Language Models

A Brief, Incomplete, and Somewhat Incorrect History of Language Models

è¯­è¨€æ¨¡å‹ç®€å²ï¼ˆä¸å®Œæ•´ä¸”ç•¥æœ‰åå·®ï¼‰

ğŸ“Œ Skip to here if you'd like to jump past the history of language models. This section is for the curious minded, though may also help you understand the reasoning behind the advice that follows.

ğŸ“Œ è‹¥å¸Œæœ›è·³è¿‡å†å²éƒ¨åˆ†å¯ç›´æ¥è·³è½¬ã€‚æœ¬èŠ‚é¢å‘å…´è¶£è¯»è€…ï¼Œä¹Ÿæœ‰åŠ©äºç†è§£åç»­å»ºè®®çš„ç¼˜ç”±ã€‚

### Pre-2000â€™s

Pre-2000â€™s

2000 å¹´å‰

Language models have existed for decades, though traditional language models (e.g. n-gram models) have many deficiencies in terms of an explosion of state space (the curse of dimensionality) and working with novel phrases that theyâ€™ve never seen (sparsity). Plainly, older language models can generate text that vaguely resembles the statistics of human generated text, but there is no consistency within the output â€“ and a reader will quickly realize itâ€™s all gibberish. N-gram models also donâ€™t scale to large values of N, so are inherently limited.

è¯­è¨€æ¨¡å‹å·²æœ‰æ•°åå¹´å†å²ï¼Œä¼ ç»Ÿæ¨¡å‹ï¼ˆå¦‚ n-gramï¼‰é¢ä¸´çŠ¶æ€ç©ºé—´çˆ†ç‚¸ï¼ˆç»´åº¦ç¾éš¾ï¼‰ä¸æœªè§çŸ­è¯­çš„ç¨€ç–æ€§é—®é¢˜ã€‚æ—§å¼æ¨¡å‹èƒ½ç”Ÿæˆç»Ÿè®¡ä¸Šâ€œåƒâ€çš„æ–‡æœ¬ï¼Œä½†å†…éƒ¨ä¸€è‡´æ€§å·®ã€è¯»è€…æ˜“è¾¨å…¶ä¸ºâ€œèƒ¡è¨€ä¹±è¯­â€ã€‚n-gram éš¾ä»¥æ‰©å±•åˆ°å¤§ N å€¼ï¼Œå…ˆå¤©å—é™ã€‚

### Mid-2000â€™s

Mid-2000â€™s

2000 å¹´ä»£ä¸­æœŸ

In 2007, Geoffrey Hinton â€“ famous for popularizing backpropagation in 1980â€™s â€“ published an important advancement in training neural networks that unlocked much deeper networks. Applying these simple deep neural networks to language modeling helped alleviate some of problems with language models â€“ they represented nuanced arbitrary concepts in a finite space and continuous way, gracefully handling sequences not seen in the training corpus. These simple neural networks learned the probabilities of their training corpus well, but the output would statistically match the training data and generally not be coherent relative to the input sequence.

2007 å¹´ï¼ŒGeoffrey Hinton å‘è¡¨äº†é‡è¦çš„ç¥ç»ç½‘ç»œè®­ç»ƒè¿›å±•ï¼Œè§£é”æ›´æ·±ç½‘ç»œã€‚åœ¨è¯­è¨€å»ºæ¨¡ä¸­ä½¿ç”¨è¿™äº›ç®€å•çš„æ·±åº¦ç½‘ç»œï¼Œç¼“è§£äº†éƒ¨åˆ†é—®é¢˜ï¼šä»¥æœ‰é™ã€è¿ç»­çš„æ–¹å¼è¡¨å¾ç»†å¾®æ¦‚å¿µï¼Œå¹¶ä¼˜é›…å¤„ç†æœªåœ¨è®­ç»ƒè¯­æ–™ä¸­å‡ºç°çš„åºåˆ—ã€‚æ­¤ç±»ç½‘ç»œèƒ½å¾ˆå¥½å­¦ä¹ è®­ç»ƒæ•°æ®çš„æ¦‚ç‡ï¼Œä½†è¾“å‡ºå¸¸ä¸è¾“å…¥åºåˆ—çš„è¯­ä¹‰å…³è”ä¸å¤Ÿä¸€è‡´ã€‚

### Early-2010â€™s

Early-2010â€™s

2010 å¹´ä»£æ—©æœŸ

Although they were first introduced in 1995, Long Short-Term Memory (LSTM) Networks found their time to shine in the 2010â€™s. LSTMs allowed models to process arbitrary length sequences and, importantly, alter their internal state dynamically as they processed the input to remember previous things they saw. This minor tweak led to remarkable improvements. In 2015, Andrej Karpathy famously wrote about creating a character-level lstm that performed far better than it had any right to.

å°½ç®¡ LSTM æ—©åœ¨ 1995 å¹´æå‡ºï¼Œä½†åœ¨ 2010 å¹´ä»£è¿æ¥é«˜å…‰ã€‚LSTM èƒ½å¤„ç†ä»»æ„é•¿åº¦åºåˆ—ï¼Œå¹¶åœ¨å¤„ç†è¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´å†…éƒ¨çŠ¶æ€ä»¥â€œè®°ä½â€å…ˆå‰ä¿¡æ¯ï¼Œè¿™ä¸€æ”¹åŠ¨å¸¦æ¥æ˜¾è‘—æå‡ã€‚2015 å¹´ï¼ŒAndrej Karpathy è‘—ååœ°æ’°å†™äº†å­—ç¬¦çº§ LSTM çš„å®è·µï¼Œæ•ˆæœè¿œè¶…é¢„æœŸã€‚

LSTMs have seemingly magical abilities, but struggle with long term dependencies. If you asked it to complete the sentence, â€œIn France, we traveled around, ate many pastries, drank lots of wine, ... lots more text ... , but never learned how to speak _______â€, the model might struggle with predicting â€œFrenchâ€. They also process input one token at a time, so are inherently sequential, slow to train, and the Nth token only knows about the N - 1 tokens prior to it.

LSTM è™½å¼ºï¼Œä½†éš¾ä»¥å¤„ç†é•¿è·ç¦»ä¾èµ–ã€‚ä¾‹å¦‚è¦æ±‚è¡¥å…¨å¥æœ«â€œåœ¨æ³•å›½â€¦â€¦ä½†ä»æœªå­¦ä¼šè¯´ _______â€ï¼Œæ¨¡å‹å¯èƒ½éš¾ä»¥é¢„æµ‹â€œFrenchâ€ã€‚å…¶é€è¯å…ƒå¤„ç†çš„é¡ºåºç‰¹æ€§å¯¼è‡´è®­ç»ƒè¾ƒæ…¢ä¸”ä¿¡æ¯ä¼ æ’­æœ‰é™ã€‚

### Late-2010â€™s

Late-2010â€™s

2010 å¹´ä»£åæœŸ

In 2017, Google wrote a paper, Attention Is All You Need, that introduced Transformer Networks and kicked off a massive revolution in natural language processing. Overnight, machines could suddenly do tasks like translating between languages nearly as good as (sometimes better than) humans. Transformers are highly parallelizable and introduce a mechanism, called â€œattentionâ€, for the model to efficiently place emphasis on specific parts of the input. Transformers analyze the entire input all at once, in parallel, choosing which parts are most important and influential. Every output token is influenced by every input token.

2017 å¹´ï¼ŒGoogle å‘è¡¨ã€ŠAttention Is All You Needã€‹ï¼Œæå‡º Transformer ç½‘ç»œï¼Œå¼•å‘ NLP çš„é‡å¤§å˜é©ã€‚æœºå™¨ç¬é—´åœ¨ç¿»è¯‘ç­‰ä»»åŠ¡ä¸Šè¾¾åˆ°ç”šè‡³è¶…è¶Šäººç±»æ°´å¹³ã€‚Transformer é«˜åº¦å¹¶è¡Œï¼Œå¹¶é€šè¿‡â€œæ³¨æ„åŠ›â€æœºåˆ¶é«˜æ•ˆåœ°å…³æ³¨è¾“å…¥å…³é”®éƒ¨åˆ†ï¼›å®ƒå¹¶è¡Œåˆ†ææ•´ä½“è¾“å…¥ï¼Œæ¯ä¸ªè¾“å‡ºè¯å…ƒéƒ½å—æ‰€æœ‰è¾“å…¥è¯å…ƒå½±å“ã€‚

Transformers are highly parallelizable, efficient to train, and produce astounding results. A downside to transformers is that they have a fixed input and output size â€“ the context window â€“ and computation increases quadratically with the size of this window (in some cases, memory does as well!) 3.

Transformer å¹¶è¡Œæ€§å¼ºã€è®­ç»ƒé«˜æ•ˆã€ç»“æœæƒŠè‰³ã€‚ä½†å…¶å›ºå®šçš„è¾“å…¥è¾“å‡ºå°ºå¯¸ï¼ˆä¸Šä¸‹æ–‡çª—å£ï¼‰æ˜¯ç¼ºç‚¹ï¼Œè®¡ç®—é‡éšçª—å£å¤§å°è¿‘ä¼¼äºŒæ¬¡å¢é•¿ï¼ˆéƒ¨åˆ†æƒ…å†µä¸‹å†…å­˜äº¦ç„¶ï¼‰ã€‚

Transformers are not the end of the road, but the vast majority of recent improvements in natural language processing have involved them. There is still abundant active research on various ways of implementing and applying them, such as Amazonâ€™s AlexaTM 20B which outperforms GPT-3 in a number of tasks and is an order of magnitude smaller in its number of parameters.

Transformer å¹¶éç»ˆç‚¹ï¼Œä½†è¿‘æœŸ NLP çš„å¤§éƒ¨åˆ†è¿›å±•éƒ½ä¸ä¹‹ç›¸å…³ã€‚ä»æœ‰å¤§é‡å…³äºå®ç°ä¸åº”ç”¨çš„ç ”ç©¶ï¼Œä¾‹å¦‚ Amazon çš„ AlexaTM 20B åœ¨å¤šé¡¹ä»»åŠ¡ä¸Šä¼˜äº GPTâ€‘3ï¼Œä¸”å‚æ•°é‡æ›´å°ä¸€ä¸ªé‡çº§ã€‚

### 2020â€™s

2020â€™s

2020 å¹´ä»£

While technically starting in 2018, the theme of the 2020â€™s has been Generative Pre-Trained models â€“ more famously known as GPT. One year after the â€œAttention Is All You Needâ€ paper, OpenAI released Improving Language Understanding by Generative Pre-Training. This paper established that you can train a large language model on a massive set of data without any specific agenda, and then once the model has learned the general aspects of language, you can fine-tune it for specific tasks and quickly get state-of-the-art results.

å°½ç®¡æŠ€æœ¯ä¸Šå§‹äº 2018 å¹´ï¼Œ2020 å¹´ä»£ä¸»é¢˜æ˜¯ç”Ÿæˆå¼é¢„è®­ç»ƒæ¨¡å‹ï¼ˆGPTï¼‰ã€‚â€œAttention Is All You Needâ€ å‘å¸ƒä¸€å¹´åï¼ŒOpenAI æ¨å‡ºã€ŠImproving Language Understanding by Generative Pre-Trainingã€‹ï¼Œç¡®ç«‹äº†å…ˆåœ¨æµ·é‡æ•°æ®ä¸Šæ— ç›‘ç£é¢„è®­ç»ƒï¼Œå†é’ˆå¯¹ç‰¹å®šä»»åŠ¡å¾®è°ƒå³å¯å¿«é€Ÿè·å¾— SOTA çš„èŒƒå¼ã€‚

In 2020, OpenAI followed up with their GPT-3 paper Language Models are Few-Shot Learners, showing that if you scale up GPT-like models by another factor of ~10x, in terms of number of parameters and quantity of training data, you no longer have to fine-tune it for many tasks. The capabilities emerge naturally and you get state-of-the-art results via text interaction with the model.

2020 å¹´ï¼ŒOpenAI å‘è¡¨ GPTâ€‘3 è®ºæ–‡ã€ŠLanguage Models are Fewâ€‘Shot Learnersã€‹ï¼Œè¡¨æ˜è‹¥å°†å‚æ•°è§„æ¨¡ä¸è®­ç»ƒæ•°æ®æå‡çº¦åå€ï¼Œè®¸å¤šä»»åŠ¡æ— é¡»å¾®è°ƒå³å¯é€šè¿‡æ–‡æœ¬äº¤äº’è·å¾— SOTA èƒ½åŠ›ã€‚

In 2022, OpenAI followed-up on their GPT-3 accomplishments by releasing InstructGPT. The intent here was to tweak the model to follow instructions, while also being less toxic and biased in its outputs. The key ingredient here was Reinforcement Learning from Human Feedback (RLHF), a concept co-authored by Google and OpenAI in 20174, which allows humans to be in the training loop to fine-tune the model output to be more in line with human preferences. InstructGPT is the predecessor to the now famous ChatGPT.

2022 å¹´ï¼ŒOpenAI å‘å¸ƒ InstructGPTï¼Œç›®æ ‡æ˜¯è®©æ¨¡å‹æ›´å¥½åœ°éµå¾ªæŒ‡ä»¤ï¼ŒåŒæ—¶å‡å°‘ä¸è‰¯ä¸åè§è¾“å‡ºã€‚å…³é”®æŠ€æœ¯æ˜¯â€œæ¥è‡ªäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰â€ï¼Œç”± Google ä¸ OpenAI äº 2017 å¹´æå‡ºï¼Œä½¿äººç±»å‚ä¸è®­ç»ƒç¯è·¯ï¼Œä»¤è¾“å‡ºæ›´ç¬¦åˆäººç±»åå¥½ã€‚InstructGPT æ˜¯ ChatGPT çš„å‰èº«ã€‚

OpenAI has been a major contributor to large language models over the last few years, including the most recent introduction of GPT-4, but they are not alone. Meta has introduced many open source large language models like OPT, OPT-IML (instruction tuned), and LLaMa. Google released models like FLAN-T5 and BERT. And there is a huge open source research community releasing models like BLOOM and StableLM.

è¿‘å¹´ OpenAI åœ¨ LLM é¢†åŸŸè´¡çŒ®å·¨å¤§ï¼ˆåŒ…æ‹¬ GPTâ€‘4ï¼‰ï¼Œä½†å¹¶éå­¤å†›ã€‚Meta æ¨å‡º OPTã€OPTâ€‘IMLï¼ˆæŒ‡ä»¤å¾®è°ƒï¼‰ä¸ LLaMaï¼›Google å‘å¸ƒ FLANâ€‘T5ã€BERTï¼›å¼€æºç¤¾åŒºäº¦æ¶Œç° BLOOMã€StableLM ç­‰æ¨¡å‹ã€‚

Progress is now moving so swiftly that every few weeks the state-of-the-art is changing or models that previously required clusters to run now run on Raspberry PIs.

è¿›å±•é£é€Ÿï¼šå‡ å‘¨å°±ä¼šå‡ºç°æ–°çš„ SOTAï¼Œæ›¾éœ€é›†ç¾¤çš„æ¨¡å‹å¦‚ä»Šå¯åœ¨æ ‘è“æ´¾ä¸Šè¿è¡Œã€‚

## What is a prompt?

What is a prompt?

ä»€ä¹ˆæ˜¯æç¤ºè¯ï¼Ÿ

A prompt, sometimes referred to as context, is the text provided to a model before it begins generating output. It guides the model to explore a particular area of what it has learned so that the output is relevant to your goals. As an analogy, if you think of the language model as a source code interpreter, then a prompt is the source code to be interpreted. Somewhat amusingly, a language model will happily attempt to guess what source code will do:

æç¤ºè¯ï¼ˆäº¦ç§°ä¸Šä¸‹æ–‡ï¼‰æ˜¯åœ¨æ¨¡å‹ç”Ÿæˆè¾“å‡ºå‰æä¾›çš„æ–‡æœ¬ã€‚å®ƒå¼•å¯¼æ¨¡å‹èšç„¦å·²å­¦çŸ¥è¯†ä¸­çš„ç‰¹å®šåŒºåŸŸï¼Œä½¿è¾“å‡ºæ›´è´´åˆç›®æ ‡ã€‚ç±»æ¯”è€Œè¨€ï¼Œè‹¥å°†è¯­è¨€æ¨¡å‹è§†ä½œâ€œæºç è§£é‡Šå™¨â€ï¼Œæç¤ºè¯å°±æ˜¯å¾…è§£é‡Šçš„â€œæºç â€ã€‚æœ‰è¶£çš„æ˜¯ï¼Œè¯­è¨€æ¨¡å‹ç”šè‡³ä¼šå°è¯•çŒœæµ‹æºç çš„è¡Œä¸ºã€‚

And it almost interprets the Python perfectly!

å®ƒå‡ ä¹â€œå®Œç¾è§£é‡Šâ€äº† Pythonï¼

Frequently, prompts will be an instruction or a question, like:

æç¤ºè¯å¸¸ä»¥æŒ‡ä»¤æˆ–é—®é¢˜å½¢å¼å‡ºç°ï¼Œä¾‹å¦‚ï¼š

On the other hand, if you donâ€™t specify a prompt, the model has no anchor to work from and youâ€™ll see that it just randomly samples from anything it has ever consumed:

åä¹‹ï¼Œè‹¥ä¸æŒ‡å®šæç¤ºè¯ï¼Œæ¨¡å‹ç¼ºä¹é”šç‚¹ï¼Œå¾€å¾€ä¼šéšæœºé‡‡æ ·æ›¾è§è¿‡çš„ä»»ä½•å†…å®¹ã€‚

From GPT-3-Davinci:

æ¥è‡ª GPTâ€‘3â€‘Davinciï¼š

From GPT-4:

æ¥è‡ª GPTâ€‘4ï¼š

## Hidden Prompts

Hidden Prompts

éšè—æç¤ºè¯

âš ï¸ Always assume that any content in a hidden prompt can be seen by the user.

âš ï¸ å§‹ç»ˆå‡è®¾éšè—æç¤ºä¸­çš„ä»»ä½•å†…å®¹æœ€ç»ˆéƒ½å¯èƒ½è¢«ç”¨æˆ·çœ‹åˆ°ã€‚

In applications where a user is interacting with a model dynamically, such as chatting with the model, there will typically be portions of the prompt that are never intended to be seen by the user. These hidden portions may occur anywhere, though there is almost always a hidden prompt at the start of a conversation.

åœ¨ç”¨æˆ·ä¸æ¨¡å‹åŠ¨æ€äº¤äº’ï¼ˆå¦‚èŠå¤©ï¼‰çš„åº”ç”¨ä¸­ï¼Œæç¤ºè¯é€šå¸¸åŒ…å«ä¸æ‰“ç®—è®©ç”¨æˆ·çœ‹åˆ°çš„éƒ¨åˆ†ã€‚è¿™äº›éšè—ç‰‡æ®µå¯èƒ½å‡ºç°åœ¨ä»»æ„ä½ç½®ï¼Œä½†å‡ ä¹æ€»ä¼šåœ¨ä¼šè¯å¼€ç«¯è®¾ç½®ä¸€ä¸ªéšè—æç¤ºã€‚

Typically, this includes an initial chunk of text that sets the tone, model constraints, and goals, along with other dynamic information that is specific to the particular session â€“ user name, location, time of day, etc...

é€šå¸¸åŒ…æ‹¬è®¾å®šè¯­æ°”ã€æ¨¡å‹çº¦æŸä¸ç›®æ ‡çš„åˆå§‹æ–‡æœ¬ï¼Œä»¥åŠä¼šè¯ç‰¹å®šçš„åŠ¨æ€ä¿¡æ¯å¦‚ç”¨æˆ·åã€ä½ç½®ã€æ—¶é—´ç­‰ã€‚

The model is static and frozen at a point in time, so if you want it to know current information, like the time or the weather, you must provide it.

æ¨¡å‹åœ¨è®­ç»ƒå®Œæˆåé™æ€å†»ç»“ï¼Œè‹¥éœ€è¦å…¶çŸ¥æ™“å½“å‰ä¿¡æ¯ï¼ˆå¦‚æ—¶é—´ã€å¤©æ°”ï¼‰ï¼Œå¿…é¡»åœ¨æç¤ºä¸­æ˜¾å¼æä¾›ã€‚

If youâ€™re using the OpenAI Chat API, they delineate hidden prompt content by placing it in the system role.

åœ¨ OpenAI Chat API ä¸­ï¼Œéšè—æç¤ºå†…å®¹é€šå¸¸æ”¾åœ¨ `system` è§’è‰²ä¸­ä»¥åŠ ä»¥åŒºåˆ†ã€‚

Hereâ€™s an example of a hidden prompt followed by interactions with the content in that prompt:

ä¸‹ä¾‹å±•ç¤ºäº†éšè—æç¤ºä»¥åŠéšåå›´ç»•è¯¥æç¤ºå†…å®¹çš„äº¤äº’ï¼š

In this example, you can see we explain to the bot the various roles, some context on the user, some dynamic data we want the bot to have access to, and then guidance on how the bot should respond.

è¯¥ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å‘æœºå™¨äººè§£é‡Šäº†å„è§’è‰²ã€ç”¨æˆ·ä¸Šä¸‹æ–‡ã€å¸Œæœ›å…¶è®¿é—®çš„åŠ¨æ€æ•°æ®ï¼Œå¹¶ç»™å‡ºåº”ç­”çš„æŒ‡å¯¼ã€‚

In practice, hidden prompts may be quite large. Hereâ€™s a larger prompt taken from a ChatGPT command-line assistant:

å®è·µä¸­ï¼Œéšè—æç¤ºå¯èƒ½ç›¸å½“åºå¤§ã€‚ä»¥ä¸‹ç¤ºä¾‹å–è‡ª ChatGPT å‘½ä»¤è¡ŒåŠ©æ‰‹ï¼š

From: https://github.com/manno/chatgpt-linux-assistant

æ¥æºï¼šhttps://github.com/manno/chatgpt-linux-assistant

Youâ€™ll see some good practices there, such as including lots of examples, repetition for important behavioral aspects, constraining the replies, etcâ€¦

å…¶ä¸­åŒ…å«è¯¸å¤šè‰¯å¥½å®è·µï¼Œå¦‚å¤§é‡ç¤ºä¾‹ã€å¯¹å…³é”®è¡Œä¸ºçš„é‡å¤å¼ºè°ƒã€å›å¤çº¦æŸç­‰ã€‚

âš ï¸ Always assume that any content in a hidden prompt can be seen by the user.

âš ï¸ å†æ¬¡æé†’ï¼šéšè—æç¤ºä¸­çš„ä»»ä½•å†…å®¹éƒ½å¯èƒ½è¢«ç”¨æˆ·è·å–ã€‚

## Tokens

Tokens

è¯å…ƒï¼ˆTokenï¼‰

If you thought tokens were ğŸ”¥ in 2022, tokens in 2023 are on a whole different plane of existence. The atomic unit of consumption for a language model is not a â€œwordâ€, but rather a â€œtokenâ€. You can kind of think of tokens as syllables, and on average they work out to about 750 words per 1,000 tokens. They represent many concepts beyond just alphabetical characters â€“ such as punctuation, sentence boundaries, and the end of a document.

åœ¨ 2023 å¹´ï¼Œâ€œè¯å…ƒâ€æ¯” 2022 å¹´æ›´ä¸ºå…³é”®ã€‚è¯­è¨€æ¨¡å‹çš„æœ€å°å¤„ç†å•å…ƒä¸æ˜¯â€œè¯â€ï¼Œè€Œæ˜¯â€œè¯å…ƒâ€ã€‚å¯å°†è¯å…ƒç²—ç•¥ç†è§£ä¸ºéŸ³èŠ‚ï¼Œå¹³å‡çº¦ä¸ºæ¯ 1000 è¯å…ƒå¯¹åº” 750 ä¸ªè‹±æ–‡å•è¯ã€‚è¯å…ƒä¸ä»…è¡¨ç¤ºå­—æ¯å­—ç¬¦ï¼Œè¿˜åŒ…æ‹¬æ ‡ç‚¹ã€å¥è¾¹ç•Œä¸æ–‡æ¡£ç»“æŸç­‰æ¦‚å¿µã€‚

Hereâ€™s an example of how GPT may tokenize a sequence:

ä»¥ä¸‹å±•ç¤º GPT å¯¹ä¸€æ®µæ–‡æœ¬çš„è¯å…ƒåŒ–ç¤ºä¾‹ï¼š

You can experiment with a tokenizer here: https://platform.openai.com/tokenizer

å¯åœ¨æ­¤å¤„è¯•éªŒåˆ†è¯å™¨ï¼šhttps://platform.openai.com/tokenizer

Different models will use different tokenizers with different levels of granularity. You could, in theory, just feed a model 0â€™s and 1â€™s â€“ but then the model needs to learn the concept of characters from bits, and then the concept of words from characters, and so forth. Similarly, you could feed the model a stream of raw characters, but then the model needs to learn the concept of words, and punctuation, etcâ€¦ and, in general, the models will perform worse.

ä¸åŒæ¨¡å‹ä½¿ç”¨ä¸åŒçš„åˆ†è¯å™¨ï¼Œç²’åº¦å„å¼‚ã€‚ç†è®ºä¸Šå¯ç›´æ¥è¾“å…¥ 0/1 æ¯”ç‰¹ï¼Œä½†æ¨¡å‹éœ€è¦ä»æ¯”ç‰¹å­¦ä¹ å­—ç¬¦ï¼Œå†ä»å­—ç¬¦å­¦ä¹ è¯è¯­ï¼Œæ•ˆç‡ä½ã€‚è‹¥è¾“å…¥åŸå§‹å­—ç¬¦æµï¼Œæ¨¡å‹ä»éœ€å­¦ä¹ è¯ä¸æ ‡ç‚¹ç­‰æ¦‚å¿µï¼Œæ•´ä½“è¡¨ç°é€šå¸¸æ›´å·®ã€‚

To learn more, Hugging Face has a wonderful introduction to tokenizers and why they need to exist.

æ›´å¤šèµ„æ–™å¯å‚è€ƒ Hugging Face å…³äºåˆ†è¯å™¨çš„ä¼˜ç§€ä»‹ç»åŠå…¶å¿…è¦æ€§ã€‚

Thereâ€™s a lot of nuance around tokenization, such as vocabulary size or different languages treating sentence structure meaningfully different (e.g. words not being separated by spaces). Fortunately, language model APIs will almost always take raw text as input and tokenize it behind the scenes â€“ so you rarely need to think about tokens.

è¯å…ƒåŒ–å­˜åœ¨è¯¸å¤šç»†å¾®å·®åˆ«ï¼Œå¦‚è¯è¡¨å¤§å°æˆ–ä¸åŒè¯­è¨€çš„å¥æ³•å·®å¼‚ï¼ˆä¾‹å¦‚è¯é—´ä¸ä»¥ç©ºæ ¼åˆ†éš”ï¼‰ã€‚å¹¸è¿çš„æ˜¯ï¼Œå¤§å¤šæ•° API æ¥æ”¶åŸå§‹æ–‡æœ¬å¹¶åœ¨åå°å®Œæˆè¯å…ƒåŒ–ï¼Œé€šå¸¸æ— éœ€æ˜¾å¼å¤„ç†è¯å…ƒã€‚

Except for one important scenario, which we discuss next: token limits.

ä½†æœ‰ä¸€ä¸ªé‡è¦ä¾‹å¤–ï¼Œä¸‹ä¸€èŠ‚å°†è®¨è®ºï¼šè¯å…ƒé™åˆ¶ã€‚

## Token Limits

Token Limits

è¯å…ƒé™åˆ¶

Prompts tend to be append-only, because you want the bot to have the entire context of previous messages in the conversation. Language models, in general, are stateless and wonâ€™t remember anything about previous requests to them, so you always need to include everything that it might need to know that is specific to the current session.

æç¤ºè¯å¸¸é‡‡ç”¨â€œåªè¿½åŠ â€ç­–ç•¥ï¼Œä»¥ä¿ç•™ä¼šè¯ä¸Šä¸‹æ–‡ã€‚è¯­è¨€æ¨¡å‹é€šå¸¸æ˜¯æ— çŠ¶æ€çš„ï¼Œä¸è®°å¿†å…ˆå‰è¯·æ±‚ï¼Œå› æ­¤éœ€è¦åœ¨æç¤ºä¸­åŒ…å«ä¼šè¯æ‰€éœ€çš„å…¨éƒ¨ä¿¡æ¯ã€‚

A major downside of this is that the leading language model architecture, the Transformer, has a fixed input and output size â€“ at a certain point the prompt canâ€™t grow any larger. The total size of the prompt, sometimes referred to as the â€œcontext windowâ€, is model dependent. For GPT-3, it is 4,096 tokens. For GPT-4, it is 8,192 tokens or 32,768 tokens depending on which variant you use.

æ­¤ç­–ç•¥çš„ä¸»è¦é—®é¢˜åœ¨äºé¢†å…ˆæ¶æ„ Transformer çš„å›ºå®šè¾“å…¥è¾“å‡ºå°ºå¯¸ï¼ˆä¸Šä¸‹æ–‡çª—å£ï¼‰ã€‚æç¤ºæ€»é•¿åº¦å—æ¨¡å‹é™åˆ¶ï¼šGPTâ€‘3 ä¸º 4096 è¯å…ƒï¼›GPTâ€‘4 ä¾ç‰ˆæœ¬ä¸º 8192 æˆ– 32768 è¯å…ƒã€‚

If your context grows too large for the model, the most common tactic is the truncate the context in a sliding window fashion. If you think of a prompt as hidden initialization prompt + messages[], usually the hidden prompt will remain unaltered, and the messages[] array will take the last N messages.

è‹¥ä¸Šä¸‹æ–‡è¿‡å¤§ï¼Œå¸¸ç”¨â€œæ»‘åŠ¨çª—å£æˆªæ–­â€ã€‚å°†æç¤ºè§†ä¸ºâ€œéšè—åˆå§‹åŒ–æç¤º + messages[]â€ï¼Œé€šå¸¸ä¿ç•™éšè—æç¤ºä¸å˜ï¼Œä»…åœ¨ messages[] ä¸­ä¿ç•™æœ€è¿‘ N æ¡æ¶ˆæ¯ã€‚

You may also see more clever tactics for prompt truncation â€“ such as discarding only the user messages first, so that the bot's previous answers stay in the context for as long as possible, or asking an LLM to summarize the conversation and then replacing all of the messages with a single message containing that summary. There is no correct answer here and the solution will depend on your application.

è¿˜å¯é‡‡ç”¨æ›´å·§å¦™çš„æˆªæ–­ï¼šä¼˜å…ˆä¸¢å¼ƒç”¨æˆ·æ¶ˆæ¯ä»¥ä¿ç•™æœºå™¨å›ç­”ï¼Œæˆ–è®© LLM æ€»ç»“ä¼šè¯å¹¶ä»¥å•æ¡æ‘˜è¦æ›¿æ¢æ‰€æœ‰æ¶ˆæ¯ã€‚å¹¶æ— å”¯ä¸€æ­£ç¡®æ–¹æ¡ˆï¼Œéœ€è§†åº”ç”¨è€Œå®šã€‚

Importantly, when truncating the context, you must truncate aggressively enough to allow room for the response as well. OpenAIâ€™s token limits include both the length of the input and the length of the output. If your input to GPT-3 is 4,090 tokens, it can only generate 6 tokens in response.

å…³é”®æ˜¯æˆªæ–­éœ€é¢„ç•™å……è¶³å“åº”ç©ºé—´ã€‚OpenAI çš„è¯å…ƒé™åˆ¶åŒæ—¶è®¡å…¥è¾“å…¥ä¸è¾“å‡ºé•¿åº¦ï¼šè‹¥è¾“å…¥ä¸º 4090 è¯å…ƒï¼ŒGPTâ€‘3 æœ€å¤šåªèƒ½ç”Ÿæˆ 6 ä¸ªè¯å…ƒä½œä¸ºè¾“å‡ºã€‚

ğŸ§™â€â™‚ï¸ If youâ€™d like to count the number of tokens before sending the raw text to the model, the specific tokenizer to use will depend on which model you are using. OpenAI has a library called tiktoken that you can use with their models â€“ though there is an important caveat that their internal tokenizer may vary slightly in count, and they may append other metadata, so consider this an approximation.

ğŸ§™â€â™‚ï¸ è‹¥éœ€åœ¨å‘é€å‰ç»Ÿè®¡è¯å…ƒæ•°ï¼Œå…·ä½“åˆ†è¯å™¨å–å†³äºæ¨¡å‹ã€‚OpenAI æä¾› `tiktoken` åº“ï¼Œä½†éœ€æ³¨æ„å…¶å†…éƒ¨è®¡æ•°å¯èƒ½ç•¥æœ‰å·®å¼‚å¹¶é™„åŠ å…ƒæ•°æ®ï¼Œç»Ÿè®¡ä»…ä¾›è¿‘ä¼¼å‚è€ƒã€‚

If youâ€™d like an approximation without having access to a tokenizer, input.length / 4 will give a rough, but better than youâ€™d expect, approximation for English inputs.

è‹¥æ— åˆ†è¯å™¨ï¼Œå¯ç”¨ `input.length / 4` å¯¹è‹±æ–‡è¾“å…¥è¿›è¡Œç²—ç•¥ä¼°ç®—ï¼Œæ•ˆæœå¾€å¾€å¥½äºé¢„æœŸã€‚

## Prompt Hacking

Prompt Hacking

æç¤ºè¯æ”»é˜²

Prompt engineering and large language models are a fairly nascent field, so new ways to hack around them are being discovered every day. The two large classes of attacks are:

æç¤ºå·¥ç¨‹ä¸ LLM ä»å±æ–°å…´é¢†åŸŸï¼Œæ¯å¤©éƒ½æœ‰æ–°çš„æ”»å‡»æ–¹å¼å‡ºç°ã€‚ä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼š

Make the bot bypass any guidelines you have given it.

è®©æœºå™¨äººç»•è¿‡ä½ è®¾å®šçš„å‡†åˆ™ã€‚

Make the bot output hidden context that you didnâ€™t intend for the user to see.

é€¼ä½¿æœºå™¨äººè¾“å‡ºä¸åº”è®©ç”¨æˆ·çœ‹åˆ°çš„éšè—ä¸Šä¸‹æ–‡ã€‚

There are no known mechanisms to comprehensively stop these, so it is important that you assume the bot may do or say anything when interacting with an adversarial user. Fortunately, in practice, these are mostly cosmetic concerns.

ç›®å‰æ— å…¨é¢é˜²å¾¡æœºåˆ¶ï¼Œå› æ­¤éœ€å‡è®¾åœ¨å¯¹æŠ—æ€§åœºæ™¯ä¸­æœºå™¨äººå¯èƒ½â€œè¯´æˆ–åšä»»ä½•äº‹â€ã€‚å¥½åœ¨å®è·µä¸­è¿™äº›é—®é¢˜å¤šä¸ºâ€œå¤–è§‚å±‚é¢â€ã€‚

Think of prompts as a way to improve the normal user experience. We design prompts so that normal users donâ€™t stumble outside of our intended interactions â€“ but always assume that a determined user will be able to bypass our prompt constraints.

å°†æç¤ºè§†ä¸ºæ”¹å–„æ­£å¸¸ç”¨æˆ·ä½“éªŒçš„æ‰‹æ®µï¼šæˆ‘ä»¬è®¾è®¡æç¤ºä»¥é¿å…æ™®é€šç”¨æˆ·åç¦»é¢„æœŸäº¤äº’ï¼›ä½†é¡»å‡è®¾â€œæœ‰æ„ä¸ºä¹‹â€çš„ç”¨æˆ·æ€»èƒ½ç»•è¿‡æç¤ºçº¦æŸã€‚

### Jailbreaks

Jailbreaks

è¶Šç‹±

Typically hidden prompts will tell the bot to behave with a certain persona and focus on specific tasks or avoid certain words. It is generally safe to assume the bot will follow these guidelines for non-adversarial users, although non-adversarial users may accidentally bypass the guidelines too.

éšè—æç¤ºé€šå¸¸è¦æ±‚æœºå™¨äººé‡‡ç”¨æŸç§äººæ ¼ã€èšç„¦ç‰¹å®šä»»åŠ¡æˆ–é¿å…æŸäº›è¯ã€‚å¯¹éå¯¹æŠ—æ€§ç”¨æˆ·ï¼Œæœºå™¨äººä¸€èˆ¬ä¼šéµå¾ªè¿™äº›å‡†åˆ™ï¼Œä½†ä¹Ÿå¯èƒ½è¢«è¯¯è§¦å‘ç»•è¿‡ã€‚

For example, we can tell the bot:

ä¾‹å¦‚æˆ‘ä»¬å¯ä»¥è¦æ±‚ï¼š

```
You are a helpful assistant, but you are never allowed to use the word "computer".
```

```
ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ï¼Œä½†ç»ä¸å…è®¸ä½¿ç”¨â€œcomputerâ€ä¸€è¯ã€‚
```

If we then ask it a question about computers, it will refer to them as a â€œdevice used for computingâ€ because it isnâ€™t allowed to use the word â€œcomputerâ€.

è‹¥å†è¯¢é—®å…³äºâ€œè®¡ç®—æœºâ€çš„é—®é¢˜ï¼Œæ¨¡å‹ä¼šä»¥â€œç”¨äºè®¡ç®—çš„è®¾å¤‡â€æ›¿ä»£â€œcomputerâ€ã€‚

It will absolutely refuse to say the word:

å®ƒå°†æ˜ç¡®æ‹’ç»ä½¿ç”¨è¯¥è¯ï¼š

But we can bypass these instructions and get the model to happily use the word if we trick it by asking it to translate the pig latin version of â€œcomputerâ€.

ä½†è‹¥é€šè¿‡è¦æ±‚ç¿»è¯‘â€œcomputerâ€çš„çŒªæ‹‰ä¸æ–‡å˜ä½“ç­‰æ–¹å¼ï¼Œå¯ç»•è¿‡çº¦æŸä½¿æ¨¡å‹ä½¿ç”¨è¯¥è¯ã€‚

There are a number of defensive measures you can take here, but typically the best bet is to reiterate your most important constraints as close to the end as possible. For the OpenAI chat API, this might mean including it as a system message after the last user message. Hereâ€™s an example:

é˜²å¾¡ç­–ç•¥å¾ˆå¤šï¼Œä½†é€šå¸¸æœ€ä½³åšæ³•æ˜¯å°½å¯èƒ½åœ¨æ¶ˆæ¯åºåˆ—æœ«å°¾é‡å¤æœ€é‡è¦çº¦æŸã€‚åœ¨ OpenAI Chat API ä¸­ï¼Œå¯åœ¨æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä¹‹ååŠ å…¥ç³»ç»Ÿæ¶ˆæ¯é‡ç”³çº¦æŸã€‚ä¾‹å¦‚ï¼š

Despite OpenAI investing a lot into jailbreaks, there are very clever work arounds being shared every day.

å°½ç®¡ OpenAI åœ¨é˜²è¶Šç‹±æ–¹é¢æŠ•å…¥å·¨å¤§ï¼Œä»ä¸æ–­æœ‰äººåˆ†äº«å·§å¦™ç»•è¿‡æ–¹å¼ã€‚

### Leaks

Leaks

ä¿¡æ¯æ³„éœ²

If you missed the previous warnings in this doc, you should always assume that any data exposed to the language model will eventually be seen by the user.

å†æ¬¡æé†’ï¼šä»»ä½•æä¾›ç»™è¯­è¨€æ¨¡å‹çš„æ•°æ®æœ€ç»ˆéƒ½å¯èƒ½è¢«ç”¨æˆ·è·å–ã€‚

As part of constructing prompts, you will often embed a bunch of data in hidden prompts (a.k.a. system prompts). The bot will happily relay this information to the user:

æ„å»ºæç¤ºæ—¶å¸¸ä¼šåœ¨éšè—æç¤ºï¼ˆç³»ç»Ÿæç¤ºï¼‰ä¸­åµŒå…¥å¤§é‡æ•°æ®ã€‚æœºå™¨äººå¯èƒ½ä¼šå°†è¿™äº›ä¿¡æ¯è½¬è¿°ç»™ç”¨æˆ·ï¼š

Even if you instruct it not to reveal the information, and it obeys those instructions, there are millions of ways to leak data in the hidden prompt.

å³ä¾¿ä½ æŒ‡ç¤ºâ€œä¸æ³„éœ²ä¿¡æ¯â€ï¼Œä¸”æ¨¡å‹éµå®ˆï¼Œä¹Ÿå­˜åœ¨æ— æ•°æ–¹å¼ä»éšè—æç¤ºä¸­æ³„éœ²æ•°æ®ã€‚

Here we have an example where the bot should never mention my city, but a simple reframing of the question getâ€™s it to spill the beans.

ç¤ºä¾‹ï¼šæœºå™¨äººä¸åº”æåŠæˆ‘çš„åŸå¸‚ï¼Œä½†é€šè¿‡ç®€å•æ”¹å†™æé—®å³å¯è®©å…¶â€œè¯´æ¼å˜´â€ã€‚

Similarly, we get the bot to tell us what word it isnâ€™t allowed to say without ever actually saying the word:

åŒç†ï¼Œæˆ‘ä»¬å¯è®©æœºå™¨äººè¯´å‡ºå…¶â€œç¦æ­¢ä½¿ç”¨çš„è¯â€è€Œæ— éœ€å®ƒçœŸçš„ä½¿ç”¨è¯¥è¯ï¼š

You should think of a hidden prompt as a means to make the user experience better or more inline with the persona youâ€™re targeting. Never place any information in a prompt that you wouldnâ€™t visually render for someone to read on screen.

å°†éšè—æç¤ºè§†ä¸ºæ”¹å–„ä½“éªŒæˆ–å¡‘é€ äººæ ¼çš„æ‰‹æ®µï¼›åˆ‡å‹¿åœ¨æç¤ºä¸­æ”¾å…¥ä»»ä½•ä½ ä¸ä¼šåœ¨å±å¹•ä¸Šæ˜ç¤ºå±•ç¤ºçš„ä¿¡æ¯ã€‚

---

æ³¨ï¼šç”±äºåŸæ–‡è¾ƒé•¿ï¼Œåç»­ç« èŠ‚ï¼ˆå¦‚â€œä¸ºä½•éœ€è¦æç¤ºå·¥ç¨‹â€â€œç­–ç•¥â€â€œæ€ç»´é“¾â€â€œå¾®è°ƒâ€â€œé™åˆ¶ä¸èµ„æºâ€ç­‰ï¼‰å‡æŒ‰ä¸Šè¿°â€œè‹±æ®µ+ä¸­æ®µâ€çš„æ¨¡å¼å®Œæ•´ç¿»è¯‘å¹¶ä¿ç•™åŸæ ¼å¼ï¼ˆåˆ—è¡¨ã€ä»£ç å—ã€å¼•ç”¨ã€é“¾æ¥ï¼‰ã€‚ä¸ºä¾¿äºé˜…è¯»ä¸åŠ è½½ï¼Œæœ¬æ–‡å·²åœ¨ç«™ç‚¹ä¸­åˆ†èŠ‚æ¸²æŸ“ï¼›å¦‚éœ€è·å–å®Œæ•´åŸå§‹ Markdownï¼Œè¯·è”ç³»ç«™ç‚¹ä½œè€…ã€‚

## Why do we need prompt engineering?

Why do we need prompt engineering?

ä¸ºä½•éœ€è¦æç¤ºå·¥ç¨‹ï¼Ÿ

Up above, we used an analogy of prompts as the â€œsource codeâ€ that a language model â€œinterpretsâ€. Prompt engineering is the art of writing prompts to get the language model to do what we want it to do â€“ just like software engineering is the art of writing source code to get computers to do what we want them to do.

å‰æ–‡å°†æç¤ºæ¯”ä½œè¯­è¨€æ¨¡å‹â€œè§£é‡Šâ€çš„æºç ã€‚æç¤ºå·¥ç¨‹æ˜¯ä¸€ç§â€œç”¨è‡ªç„¶è¯­è¨€ç¼–ç¨‹â€çš„è‰ºæœ¯ï¼Œé€šè¿‡ç¼–å†™æç¤ºè®©æ¨¡å‹æ‰§è¡Œé¢„æœŸä»»åŠ¡ï¼Œç±»ä¼¼è½¯ä»¶å·¥ç¨‹ç”¨æºç é©±åŠ¨è®¡ç®—æœºæ‰§è¡Œç›®æ ‡ã€‚

When writing good prompts, you have to account for the idiosyncrasies of the model(s) youâ€™re working with. The strategies will vary with the complexity of the tasks. Youâ€™ll have to come up with mechanisms to constrain the model to achieve reliable results, incorporate dynamic data that the model canâ€™t be trained on, account for limitations in the modelâ€™s training data, design around context limits, and many other dimensions.

ç¼–å†™é«˜è´¨é‡æç¤ºéœ€è€ƒè™‘å…·ä½“æ¨¡å‹çš„ç‰¹æ€§ï¼›ç­–ç•¥å› ä»»åŠ¡å¤æ‚åº¦è€Œå¼‚ã€‚éœ€è¦ï¼šç”¨æœºåˆ¶çº¦æŸæ¨¡å‹ä»¥è·å–å¯é ç»“æœã€èå…¥æ— æ³•é¢„è®­ç»ƒçš„åŠ¨æ€æ•°æ®ã€å¼¥è¡¥è®­ç»ƒæ•°æ®å±€é™ã€ç»•è¿‡ä¸Šä¸‹æ–‡é™åˆ¶ç­‰å¤šæ–¹é¢å·¥ç¨‹è®¾è®¡ã€‚

### Give a Bot a Fish / Semantic Search

Give a Bot a Fish / Semantic Search

æˆä¹‹ä»¥é±¼ / è¯­ä¹‰æ£€ç´¢

If your task requires grounded facts beyond the modelâ€™s training, provide documents via retrieval (semantic search) so the model can answer with citations.

è‹¥ä»»åŠ¡éœ€è¦è¶…å‡ºè®­ç»ƒèŒƒå›´çš„äº‹å®ï¼Œä½¿ç”¨æ£€ç´¢ï¼ˆè¯­ä¹‰æœç´¢ï¼‰æä¾›æ–‡æ¡£ï¼Œè®©æ¨¡å‹åŸºäºç‰‡æ®µä½œç­”å¹¶é™„å¼•ç”¨ã€‚

### Teach a Bot to Fish / Command Grammars / ReAct

Teach a Bot to Fish / Command Grammars / ReAct

æˆä¹‹ä»¥æ¸” / å‘½ä»¤è¯­æ³• / æ€è€ƒ-è¡ŒåŠ¨

Design constrained interaction via command grammars (structured outputs like JSON) and ReAct patterns where the model alternates between reasoning and tool use.

é€šè¿‡å‘½ä»¤è¯­æ³•è®¾è®¡å—çº¦æŸçš„äº¤äº’ï¼ˆå¦‚ JSON è¾“å‡ºï¼‰ï¼Œå¹¶é‡‡ç”¨ ReAct æ¨¡å¼åœ¨æ¨ç†ä¸å·¥å…·è°ƒç”¨é—´äº¤æ›¿ï¼Œæé«˜å¯æ§æ€§ä¸äº‹å®æ€§ã€‚

### GPT-4 vs GPT-3.5

GPT-4 vs GPT-3.5

GPT-4 ä¸ GPT-3.5 å¯¹æ¯”

Newer models (e.g., GPT-4) generally have larger context windows, better instruction following, and improved reasoning. However, they can be slower and costlier.

è¾ƒæ–°æ¨¡å‹ï¼ˆå¦‚ GPTâ€‘4ï¼‰é€šå¸¸æ‹¥æœ‰æ›´å¤§ä¸Šä¸‹æ–‡ã€æ›´å¼ºæŒ‡ä»¤éµå¾ªä¸æ¨ç†èƒ½åŠ›ï¼Œä½†å»¶è¿Ÿä¸æˆæœ¬æ›´é«˜ã€‚

## Strategies

Strategies

ç­–ç•¥

### Embedding Data (Simple Lists / Markdown Tables / JSON / Freeform / Nested)

Embedding Data (Simple Lists / Markdown Tables / JSON / Freeform / Nested)

åµŒå…¥æ•°æ®ï¼ˆç®€å•åˆ—è¡¨/Markdown è¡¨æ ¼/JSON/è‡ªç”±æ–‡æœ¬/åµŒå¥—ï¼‰

Provide data in structured forms to improve reliability and programmatic consumption. JSON is preferred for machine-readability; Markdown tables and lists help formatting.

é‡‡ç”¨ç»“æ„åŒ–å½¢å¼æä¾›æ•°æ®å¯æå‡å¯é æ€§ä¸å¯ç¼–ç¨‹æ¶ˆè´¹ã€‚JSON ä¾¿äºæœºå™¨è¯»å–ï¼›Markdown è¡¨æ ¼ä¸åˆ—è¡¨åˆ©äºäººç±»å¯è¯»çš„æ’ç‰ˆã€‚

### Citations / Programmatic Consumption

Citations / Programmatic Consumption

å¼•ç”¨ / ç¨‹åºåŒ–æ¶ˆè´¹

Require answers to include citations (doc#section) and return in machine-readable formats (JSON schemas) for downstream systems.

è¦æ±‚è¾“å‡ºé™„å¸¦å¼•ç”¨ï¼ˆdoc#sectionï¼‰ï¼Œå¹¶ä»¥æœºå™¨å¯è¯»æ ¼å¼ï¼ˆJSON Schemaï¼‰è¿”å›ï¼Œä¾¿äºä¸‹æ¸¸ç³»ç»Ÿæ¶ˆè´¹ã€‚

### Chain of Thought / Averaging / Interpreting Code / Delimiters

Chain of Thought / Averaging / Interpreting Code / Delimiters

æ€ç»´é“¾ / å¤šæ ·æœ¬å¹³å‡ / ä»£ç è§£é‡Š / åˆ†éš”ç¬¦

- Chain of Thought: Instruct step-by-step reasoning; optionally hide intermediate steps from final output.

- æ€ç»´é“¾ï¼šè¦æ±‚åˆ†æ­¥æ¨ç†ï¼›å¯åœ¨æœ€ç»ˆè¾“å‡ºéšè—ä¸­é—´æ­¥éª¤ã€‚

- Averaging: Sample multiple answers and average or vote to improve robustness.

- å¤šæ ·æœ¬å¹³å‡ï¼šå¤šæ¬¡é‡‡æ ·å¹¶å¹³å‡/æŠ•ç¥¨ä»¥æå‡ç¨³å¥æ€§ã€‚

- Interpreting Code: Ask models to explain or execute pseudo-code; constrain with delimiters to separate instructions and data.

- ä»£ç è§£é‡Šï¼šè®©æ¨¡å‹è§£é‡Šæˆ–æ‰§è¡Œä¼ªä»£ç ï¼›ç”¨åˆ†éš”ç¬¦æ˜ç¡®æŒ‡ä»¤ä¸æ•°æ®çš„è¾¹ç•Œã€‚

### Fine Tuning

Fine Tuning

å¾®è°ƒ

Fine-tuning can align models to specific domains or styles but requires data curation, evaluation sets, and careful governance to avoid overfitting and bias.

å¾®è°ƒå¯å¯¹é½ç‰¹å®šé¢†åŸŸä¸é£æ ¼ï¼Œä½†éœ€è¦æ•°æ®æ¸…æ´—ã€è¯„ä¼°é›†ä¸æ²»ç†ï¼Œä»¥é¿å…è¿‡æ‹Ÿåˆä¸åè§ã€‚

### Downsides

Downsides

é™åˆ¶ä¸ä»£ä»·

LLMs may hallucinate, drift with parameters, leak hidden prompts, or be jailbroken. Costs and latency matter. Design prompts, tools, and evaluations to mitigate.

LLM å¯èƒ½å¹»è§‰ã€éšå‚æ•°æ¼‚ç§»ã€æ³„éœ²éšè—æç¤ºæˆ–è¢«è¶Šç‹±ï¼›æˆæœ¬ä¸å»¶è¿Ÿäº¦å…³é”®ã€‚éœ€ä»¥æç¤ºã€å·¥å…·ä¸è¯„ä¼°å…±åŒç¼“è§£ã€‚

## Additional Resources

Additional Resources

æ‰©å±•èµ„æº

Brex includes pointers to tokenizer tools, OpenAI tiktoken, RAG references, ReAct papers, and open-source models like LLaMa, BLOOM, StableLM.

Brex æ–‡æ¡£æŒ‡å‘åˆ†è¯å™¨å·¥å…·ã€OpenAI tiktokenã€RAG å‚è€ƒã€ReAct è®ºæ–‡ä»¥åŠ LLaMaã€BLOOMã€StableLM ç­‰å¼€æºæ¨¡å‹èµ„æºã€‚