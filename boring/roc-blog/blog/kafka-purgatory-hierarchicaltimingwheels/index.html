<!doctype html><html class=scroll-smooth lang=en><head><meta charset=utf-8><link href=/css/style.css rel=stylesheet><link href=/line-awesome/css/line-awesome.min.css rel=stylesheet><script defer src=/js/main.js></script><title>Apache Kafka, Purgatory, and Hierarchical Timing Wheels | Roc's Blog</title><body class="bg-white dark:bg-slate-900 transition ease-in-out"><section><div class="sticky top-0 bg-slate-100 dark:bg-slate-800"><div class="container mx-auto px-auto xl:px-0 w-full xl:w-1/2 flex place-content-between py-16 xl:py-8 font-sans text-6xl xl:text-2xl text-slate-900 dark:text-slate-300"><div class=flex><a class="m-0 p-0 text-slate-900 hover:text-slate-700 dark:text-slate-300 dark:hover:text-slate-200" href=/blog/> /blog </a></div><div class="flex gap-4"><div class="hidden cursor-pointer" id=back-to-top><i class="las la-level-up-alt"></i></div><a href=/><i class="las la-home"></i></a><div class=cursor-pointer id=darkmode-toggle><div class="hidden dark:inline"><i class="las la-sun"></i></div><div class="inline dark:hidden"><i class="las la-moon"></i></div></div></div></div></div><div class="container mx-auto w-full xl:w-1/2 mb-16"><div class="mt-4 font-serif text-slate-600 dark:text-slate-500 text-4xl xl:text-base">2023-11-12</div><h1 class="w-full xl:w-2/3 mt-4 mb-8 font-serif text-8xl xl:text-4xl text-slate-900 dark:text-slate-300">Apache Kafka, Purgatory, and Hierarchical Timing Wheels</h1><div class="w-100 border-t mb-8 border-slate-300 dark:border-slate-700"></div><div class="prose dark:prose-invert prose-pre:rounded-none prose-headings:bg-amber-100 prose-headings:text-slate-800 dark:prose-headings:bg-indigo-900 prose-headings:font-normal dark:prose-headings:text-slate-300 prose-headings:p-2 prose-headings:w-max prose-headings:font-serif prose-2xl xl:prose-base"><p>Kafka 中有很多HTTP请求需要处理，例如：生产者 ack=all、消费者 min.bytes=1 、集群内部 broker 之间通信，需要跟踪这些请求的状态（成功、失败、超时）；<p>Kafka 会同时有成千上万的异步请求操作同时进行，面对大规模的请求处理，使用 DelayQueue 去跟踪请求状态会有以下问题：<ul><li>请求完成后不能立即删除，需要在后续检查，检查完成的请求，触发删除操作，如果检查删除操作跟不上请求速度，会出现OOM错误；<li>开启一个单独的线程去扫描 DelayQueue，但是频繁的扫描很多 DelayQueue，会有很大的性能损耗（<strong>DelayQueue 插入和删除 O(n)</strong>）；</ul><p>新设计的目标是允许立即删除已完成的请求，并大大减少昂贵的清除过程的负载。 它需要对计时器和请求中的条目进行交叉引用。 另外，由于每个请求/完成都会执行插入/删除操作，因此强烈希望具有 O (1)插入/删除开销。<ul><li>Doubly Linked List for Buckets in Timing Wheels (时间轮中桶的双向链表实现，双向链表的优点是，允许O(1)插入/删除一项)<li>Driving Clock using DelayQueue (使用 DelayQueue 驱动时间轮，避免时间轮空转消耗资源)<li>Purging Watcher Lists (已完成的请求将立即从计时器队列中删除，O(1)开销)</ul><hr><blockquote><p>原文地址 <a href=https://www.confluent.io/blog/apache-kafka-purgatory-hierarchical-timing-wheels/>Apache Kafka, Purgatory, and Hierarchical Timing Wheels</a></blockquote><p><a href=https://www.confluent.io/blog/stream-data-platform-1/>Apache Kafka</a> has a data structure called the “request purgatory”. The <a href=https://en.wikipedia.org/wiki/Purgatory>purgatory</a> holds any request that hasn’t yet met its criteria to succeed but also hasn’t yet resulted in an error. The problem is “How can we efficiently keep track of tens of thousands of requests that are being asynchronously satisfied by other activity in the cluster?”<p>Kafka implements several request types that cannot immediately be answered with a response. Examples:<ul><li>A produce request with acks=all cannot be considered complete until all in-sync replicas have acknowledged the write and we can guarantee it will not be lost if the leader fails.<li>A fetch request with min.bytes=1 won’t be answered until there is at least one new byte of data for the consumer to consume. This allows a “long poll” so that the consumer need not busy wait checking for new data to arrive.</ul><p>These requests are considered complete when either (a) the criteria they requested is complete or (b) some timeout occurs.<p>The number of these asynchronous operations in flight at any time scales with the number of connections, which, for Kafka, is often tens of thousands.<p>The request purgatory is designed for such a large scale request handling, but the old implementation had a number of deficiencies.<p>In this blog, I would like to explain the problem with the old implementation and how the new implementation solved it. I will also present benchmark results.<h2 id=old-purgatory-design>Old Purgatory Design</h2><p>The request purgatory consists of a timeout timer and a hash map of watcher lists for event driven processing. A request is put into the purgatory when it is not immediately satisfiable because of unmet conditions. A request in the purgatory is completed later when the conditions are met or is forced to be completed (timeout) when it passed beyond the time specified in the timeout parameter of the request. In the old design, it used Java <em>DelayQueue</em> to implement the timer.<p>When a request is completed, the request is not deleted from the timer or watcher lists immediately. Instead, completed requests are deleted as they were found during condition checking. When the deletion does not keep up, the server may exhaust JVM heap and cause <em>OutOfMemoryError</em>.<p>To alleviate the situation, a separate thread, called the reaper thread, purges completed requests from the purgatory when the number of requests (either pending or completed) in the purgatory exceeds the configured number. The purge operation scans the timer queue and all watcher lists to find completed requests and deletes them.<p>By setting this configuration parameter low, the server can virtually avoid the memory problem. However, the server must pay a significant performance penalty if it scans all lists too frequently.<h2 id=new-purgatory-design>New Purgatory Design</h2><p>The goal of the new design is to allow immediate deletion of a completed request and reduce the load of expensive purge process significantly. It requires cross referencing of entries in the timer and the requests. Also it is strongly desired to have O(1) insert/delete cost since insert/delete operation happens for each request/completion.<p>To satisfy these requirements, we designed a new purgatory implementation based on <a href=http://www.cs.columbia.edu/~nahum/w6998/papers/ton97-timing-wheels.pdf>Hierarchical Timing Wheels</a> [1].<h3 id=hierarchical-timing-wheel>Hierarchical Timing Wheel</h3><p>A simple timing wheel is a circular list of buckets of timer tasks. Let u be the time unit. A timing wheel with size <strong>n</strong> has <strong>n</strong> buckets and can hold timer tasks in <strong>n * u</strong> time interval. Each bucket holds timer tasks that fall into the corresponding time range. At the beginning, the first bucket holds tasks for <strong>[0, u)</strong>, the second bucket holds tasks for <strong>[u, 2u)</strong>, …, the <strong>n</strong>-th bucket for <strong>[u * (n -1), u * n)</strong>. Every interval of time unit <strong>u</strong>, the timer ticks and moved to the next bucket then expire all timer tasks in it. So, the timer never inserts a task into the bucket for the current time since it is already expired. The timer immediately runs the expired task. The emptied bucket is then available for the next round, so if the current bucket is for the time t, it becomes the bucket for <strong>[t + u * n, t + (n + 1) * u)</strong> after a tick. A timing wheel has O(1) cost for insert/delete (start-timer/stop-timer) whereas priority queue based timers, such as <em>java.util.concurrent.DelayQueue</em> and <em>java.util.Timer</em>, have O(log n) insert/delete cost. Note that neither <em>DelayQueue</em> or <em>Timer</em> supports random delete.<p><img alt src=https://cdn.confluent.io/wp-content/uploads/2016/08/TimingWheels1.png><p>A major drawback of a simple timing wheel is that it assumes that a timer request is within the time interval of <strong>n * u</strong> from the current time. If a timer request is out of this interval, it is an overflow. A hierarchical timing wheel deals with such overflows. It is a hierarchically organized timing wheels that delegate overflows to upper level wheels. The lowest level has the finest time resolution. Time resolutions become coarser as we move up the hierarchy. If the resolution of a wheel at one level is <strong>u</strong> and the size is <strong>n</strong>, the resolutions should be <strong>n * u</strong> in the second level, <strong>n2 * u</strong> in the third level, and so on. At each level overflows are delegated to the wheel in one level higher. When the wheel in the higher level ticks, it reinsert timer tasks to the lower level. An overflow wheel can be created on-demand. When a bucket in an overflow bucket expires, all tasks in it are reinserted into the timer recursively. The tasks are then moved to the finer grain wheels or be executed. The insert (start-timer) cost is O(m) where <strong>m</strong> is the number of wheels, which is usually very small compared to the number of requests in the system, and the delete (stop-timer) cost is still O(1).<p><img alt src=https://cdn.confluent.io/wp-content/uploads/2016/08/TimingWheels2.png><h3 id=doubly-linked-list-for-buckets-in-timing-wheels>Doubly Linked List for Buckets in Timing Wheels</h3><p>In the new design, we use own implementation of doubly linked list for the buckets in a timing wheel. The advantage of doubly linked list that it allows O(1) insert/delete of a list item if we have access link cells in a list.<p>A timer task instance saves a link cell in itself when enqueued to a timer queue. When a task is completed or canceled, the list is updated using the link cell saved in the task itself.<h3 id=driving-clock-using-delayqueue>Driving Clock using DelayQueue</h3><p>A simple implementation may use a thread that wakes up every unit time and does the ticking, which checks if there is any task in the bucket. The unit time of the purgatory is 1ms (<strong>u</strong> = 1ms). This can be wasteful if requests are sparse at the wheel at the lowest level. This is usually the case because the majority of requests are satisfied before inserted into the wheel at the lowest level. It would be nice if a thread wakes up only when there is a non-empty bucket to expire. The new purgatory does so by using <em>java.util.concurrent.DelayQueue</em> similarly to the old implementation, but we enqueue task buckets instead of individual tasks. This design has a performance advantage. The number of items in <em>DelayQueue</em> is capped by the number of buckets, which is usually much smaller than the number of tasks, thus the number of offer/poll operations to the priority queue inside <em>DelayQueue</em> will be significantly smaller.<h3 id=purging-watcher-lists>Purging Watcher Lists</h3><p>In the old implementation, the purge operation of watcher lists is triggered by the total size if the watcher lists. The problem is that the watcher lists may exceed the threshold even when there isn’t many requests to purge. When this happens it increases the CPU load a lot. Ideally, the purge operation should be triggered by the number of completed requests the watcher lists.<p>In the new design, a completed request is removed from the timer queue immediately with O(1) cost. It means that the number of requests in the timer queue is the number of pending requests exactly at any time. So, if we know the total number of distinct requests in the purgatory, which includes the sum of the number of pending request and the numbers completed but still watched requests, we can avoid unnecessary purge operations. It is not trivial to keep track of the exact number of distinct requests in the purgatory because a request may or may not be watched. In the new design, we estimate the total number of requests in the purgatory rather than trying to maintain the exactly number.<p>The estimated number of requests are maintained as follows.<ol><li>The estimated total number of requests, <strong>E</strong>, is incremented whenever a new request is watched.<li>Before starting the purge operation, we reset the estimated total number of requests to the size of timer queue. This is the current number of pending requests. If no requests are added to the purgatory during purge, <strong>E</strong> is the correct number of requests after purge.<li>If some requests are added to the purgatory during purge, <strong>E</strong> is incremented to <strong>E + the number of newly watched requests</strong>. This may be an overestimation because it is possible that some of the new requests are completed and remove from the watcher lists during the purge operation. We expect the chance of overestimation and an amount of overestimation are small.</ol><h2 id=benchmark>Benchmark</h2><p>We compared the enqueue performance of two purgatory implementations, the old implementation and the new implementation. This is a micro benchmark. It measures just the purgatory enqueue performance. The purgatory was separated from the rest of the system and also uses a fake request which does nothing useful. So, the throughput of the purgatory in a real system may be lower than the number shown by the test.<p>In the test, the intervals of the requests are assumed to follow the exponential distribution. Each request takes a time drawn from a log-normal distribution. By adjusting the shape of the log-normal distribution, we can test different timeout rate.<p>The tick size is 1ms and the wheel size is 20. The timeout was set to 200ms. The data size of a request was 100 bytes. For a low timeout rate case, we chose 75percentile = 60ms and 50percentile = 20. And for a high timeout rate case, we chose 75percentile = 400ms and 50percentile = 200ms. Total 1 million requests are enqueued in each run.<p>Requests are actively completed by a separate thread. Requests that are supposed to be completed before timeout are enqueued to another <em>DelayQueue</em>. And a separate thread keeps polling and completes them. There is no guarantee of accuracy in terms of actual completion time.<p>The JVM heap size is set to 200m to reproduce a memory tight situation.<p>The result shows a dramatic difference in a high enqueue rate area. As the target rate increases, both implementations keep up with the requests initially. However, in low timeout scenario the old implementation was saturated around 40000 RPS (request per second), whereas the new implementation didn’t show any significant performance degradation, and in high timeout scenario the old implementation was saturated around 25000 RPS, whereas the new implementation was saturated 105000 RPS in this benchmark.<img alt src=https://cdn.confluent.io/wp-content/uploads/2016/08/enq_rate.png><p>Also, CPU usage is significantly better in the new implementation. Note that the old implementation does not have data point higher than ~40000 RPS due to its scalability limit. Also notice that its CPU time saturates around 1.2 while it is steadily going up in the new implementation. It indicate that the old implementation may be hitting a concurrency issue due to synchronizations.<img alt src=https://cdn.confluent.io/wp-content/uploads/2016/08/cms_time.png><p>Finally, we measured total GC time (milliseconds) for <em>ParNew collection</em> and <em>CMS collection</em>. There isn’t much difference in the old implementation and the new implementation in the region of enqueue rate that the old implementation can sustain. Again note that old implementation does not have data point higher than ~40000 RPS due to its scalability limit.<img alt src=https://cdn.confluent.io/wp-content/uploads/2016/08/parnew_time.png><p><img alt src=https://cdn.confluent.io/wp-content/uploads/2016/08/cms_time.png><h2 id=summary>Summary</h2><p>In the new design, we use Hierarchical Timing Wheels for the timeout timer and <em>DelayQueue</em> of timer buckets to advance the clock on demand. Completed requests are removed from the timer queue immediately with O(1) cost. The buckets remain in the delay queue, however, the number of buckets is bounded. And, in a healthy system, most of the requests are satisfied before timeout, and many of the buckets become empty before pulled out of the delay queue. Thus, the timer should rarely have the buckets of the lower interval. The advantage of this design is that the number of requests in the timer queue is the number of pending requests exactly at any time. This allows us to estimate the number of requests need to be purged. We can avoid unnecessary purge operation of the watcher lists. As the result we achieve a higher scalability in terms of request rate with much better CPU usage.<h2 id=references>References</h2><p>[1] <a href=http://www.cs.columbia.edu/~nahum/w6998/papers/ton97-timing-wheels.pdf>George Varghese , Anthony Lauck, Hashed and hierarchical timing wheels: efficient data structures for implementing a timer facility, IEEE/ACM Transactions on Networking (TON), v.5 n.6, p.824-834, Dec. 1997</a></div></div></section>