<!doctype html><html class=scroll-smooth lang=en><head><meta charset=utf-8><link href=/css/style.css rel=stylesheet><link href=/line-awesome/css/line-awesome.min.css rel=stylesheet><script defer src=/js/main.js></script><title>基于 Filebeat、Kafka、Flink、ES、ClickHouse、HBase 的日志系统建设 | 码农的自留地</title><body class="bg-white dark:bg-slate-900 transition ease-in-out"><section><div class="sticky top-0 bg-slate-100 dark:bg-slate-800"><div class="container mx-auto px-auto xl:px-0 w-full xl:w-1/2 flex place-content-between py-16 xl:py-8 font-sans text-6xl xl:text-2xl text-slate-900 dark:text-slate-300"><div class=flex><a class="m-0 p-0 text-slate-900 hover:text-slate-700 dark:text-slate-300 dark:hover:text-slate-200" href=/blog/> /blog </a></div><div class="flex gap-4"><div class="hidden cursor-pointer" id=back-to-top><i class="las la-level-up-alt"></i></div><a href=/><i class="las la-home"></i></a><div class=cursor-pointer id=darkmode-toggle><div class="hidden dark:inline"><i class="las la-sun"></i></div><div class="inline dark:hidden"><i class="las la-moon"></i></div></div></div></div></div><div class="container mx-auto w-full xl:w-1/2 mb-16"><div class="mt-4 font-serif text-slate-600 dark:text-slate-500 text-4xl xl:text-base">2025-10-25</div><h1 class="w-full xl:w-2/3 mt-4 mb-8 font-serif text-8xl xl:text-4xl text-slate-900 dark:text-slate-300">基于 Filebeat、Kafka、Flink、ES、ClickHouse、HBase 的日志系统建设</h1><div class="mt-2 mb-6 flex flex-wrap gap-2"></div><div class="w-100 border-t mb-8 border-slate-300 dark:border-slate-700"></div><div class="prose dark:prose-invert prose-pre:rounded-none prose-headings:bg-amber-100 prose-headings:text-slate-800 dark:prose-headings:bg-indigo-900 prose-headings:font-normal dark:prose-headings:text-slate-300 prose-headings:p-2 prose-headings:w-max prose-headings:font-serif prose-2xl xl:prose-base"><h1 id=gai-lan>概览</h1><ul><li>目标：构建可扩展、低延迟、可检索与可分析的日志平台，支持实时处理与历史归档。<li>数据流：<code>Filebeat → Kafka → Flink → {Elasticsearch | ClickHouse | HBase}</code>。<li>架构图： <img alt=系统架构 src=/log-system-architecture.svg></ul><h1 id=1-xi-tong-jia-gou-she-ji>1. 系统架构设计</h1><ul><li>角色定位 <ul><li>Filebeat：轻量采集器，支持多源文件、容器日志，提供行聚合、过滤与缓冲，向 Kafka 可靠投递。<li>Kafka：日志总线与缓冲层，解耦生产与消费，提供分区并行与持久化，作为 Flink Source。<li>Flink：实时计算引擎，做清洗、解析、路由与指标聚合，Exactly-Once 输出至各存储。<li>Elasticsearch：面向检索与可视化（Kibana），适合结构化/半结构化日志快速查询。<li>ClickHouse：面向多维分析与聚合（OLAP），高吞吐写入与秒级分析，适合报表与趋势洞察。<li>HBase：长周期明细存储与宽表查询，支撑低频但深度明细回溯。</ul><li>高可用与容错 <ul><li>Filebeat：内置重试与持久队列（spool）；Kafka 端启用 <code>acks=all</code> 与幂等写。<li>Kafka：Broker 多副本+ISR，控制器自动选举；跨机房可启用 MirrorMaker2。<li>Flink：Checkpoint+Savepoint，RocksDB 状态后端，事务性 Sink（两阶段提交）。<li>Elasticsearch：索引副本与 ILM 滚动；跨集群搜索可做容灾；<li>ClickHouse：分片+副本，ReplicatedMergeTree，失败节点自动恢复；<li>HBase：HDFS 冗余、RegionServer 自动迁移，ZK 保证一致性。</ul></ul><h1 id=2-ji-shu-xuan-xing-fen-xi>2. 技术选型分析</h1><ul><li>选择考量与对比 <ul><li>Filebeat vs Fluent Bit：生态与可维护性、配置一致性；FB 多模块成熟，FB→Kafka 链路稳定。<li>Kafka vs Pulsar：Kafka 在日志生态与 Flink Source 上成熟；事务性写与生态工具完备。<li>Flink vs Spark Streaming：更低延迟与原生事件时间，Exactly-Once 更易实现。<li>ES vs OpenSearch：社区与商业支持综合评估；选 ES 8.x 以简化部署与安全集成。<li>ClickHouse vs Druid：写入吞吐与查询延迟综合更优，维护成本低；<li>HBase vs Cassandra：基于 HDFS 的生态整合与线性扩展、适合明细长保留。</ul><li>版本建议与兼容性 <ul><li>Kafka 3.x（带 KRaft 或 ZK）、Flink ≥1.17、ES 8.x、ClickHouse ≥23.x、HBase 2.x、JDK 17。<li>序列化：JSON 起步，推荐 Avro/Protobuf+Schema Registry，便于演进与兼容。</ul><li>性能基准（示例环境：8C32G+NVMe，千兆网） <ul><li>Kafka 单 Broker 吞吐（LZ4 压缩，批量写）：≥150k msg/s（1KB）；<li>Flink 端到端延迟（解析+路由）：p95 ≤ 300ms；<li>ES 索引速率：单节点 ≥20k doc/s（bulk 5k，刷新 5s）；<li>ClickHouse 插入：≥150MB/s（HTTP batch）；<li>HBase Put：≥50k row/s（批量 1k，Async 客户端）。</ul></ul><h1 id=3-he-xin-shi-xian-xi-jie>3. 核心实现细节</h1><ul><li>Filebeat 配置</ul><pre class=giallo style=color:#d8dee9;background-color:#2e3440><code data-lang=yaml><span class=giallo-l><span style=color:#8fbcbb>filebeat.inputs</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#eceff4>  -</span><span style=color:#8fbcbb> type</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> log</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    paths</span><span style=color:#eceff4>: ["</span><span style=color:#a3be8c>/var/log/app/*.log</span><span style=color:#eceff4>"]</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    multiline.pattern</span><span style=color:#eceff4>: '</span><span style=color:#a3be8c>^\[</span><span style=color:#eceff4>'</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    multiline.negate</span><span style=color:#eceff4>:</span><span style=color:#81a1c1> true</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    multiline.match</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> after</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    fields</span><span style=color:#eceff4>: {</span><span style=color:#8fbcbb>app</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> myapp</span><span style=color:#eceff4>,</span><span style=color:#8fbcbb> env</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> prod</span><span style=color:#eceff4>}</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    processors</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#eceff4>      -</span><span style=color:#8fbcbb> drop_event</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>          when</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>            equals</span><span style=color:#eceff4>: {</span><span style=color:#8fbcbb>log.level</span><span style=color:#eceff4>: "</span><span style=color:#a3be8c>debug</span><span style=color:#eceff4>"}</span></span>
<span class=giallo-l><span style=color:#8fbcbb>output.kafka</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>  hosts</span><span style=color:#eceff4>: ["</span><span style=color:#a3be8c>kafka1:9092</span><span style=color:#eceff4>","</span><span style=color:#a3be8c>kafka2:9092</span><span style=color:#eceff4>"]</span></span>
<span class=giallo-l><span style=color:#8fbcbb>  topic</span><span style=color:#eceff4>: "</span><span style=color:#a3be8c>logs.raw</span><span style=color:#eceff4>"</span></span>
<span class=giallo-l><span style=color:#8fbcbb>  compression</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> lz4</span></span>
<span class=giallo-l><span style=color:#8fbcbb>  partition.round_robin</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    reachable_only</span><span style=color:#eceff4>:</span><span style=color:#81a1c1> true</span></span>
<span class=giallo-l><span style=color:#8fbcbb>  required_acks</span><span style=color:#eceff4>:</span><span style=color:#b48ead> -1</span></span>
<span class=giallo-l><span style=color:#8fbcbb>  bulk_max_size</span><span style=color:#eceff4>:</span><span style=color:#b48ead> 2048</span></span></code></pre><ul><li>Kafka 主题规划 <ul><li>命名：<code>logs.{env}.{app}.{type}</code>；分区数按峰值吞吐与消费者并行度估算；RF≥3。<li>Producer：<code>acks=all</code>、<code>enable.idempotence=true</code>、<code>linger.ms=5-20</code>、<code>batch.size=64-256KB</code>。<li>保留：热数据 7-14 天，冷归档转对象存储或 CH/HBase。</ul><li>Flink 处理主线（Java 示例）</ul><pre class=giallo style=color:#d8dee9;background-color:#2e3440><code data-lang=java><span class=giallo-l><span style=color:#8fbcbb>StreamExecutionEnvironment</span><span> env</span><span style=color:#81a1c1> =</span><span> StreamExecutionEnvironment</span><span style=color:#eceff4>.</span><span style=color:#88c0d0>getExecutionEnvironment</span><span style=color:#eceff4>()</span><span style=color:#81a1c1>;</span></span>
<span class=giallo-l><span>env</span><span style=color:#eceff4>.</span><span style=color:#88c0d0>enableCheckpointing</span><span style=color:#eceff4>(</span><span style=color:#b48ead>30000</span><span style=color:#eceff4>)</span><span style=color:#81a1c1>;</span></span>
<span class=giallo-l><span style=color:#8fbcbb>Properties</span><span> props</span><span style=color:#81a1c1> = new</span><span style=color:#88c0d0> Properties</span><span style=color:#eceff4>()</span><span style=color:#81a1c1>;</span></span>
<span class=giallo-l><span>props</span><span style=color:#eceff4>.</span><span style=color:#88c0d0>put</span><span style=color:#eceff4>("</span><span style=color:#a3be8c>bootstrap.servers</span><span style=color:#eceff4>", "</span><span style=color:#a3be8c>kafka1:9092</span><span style=color:#eceff4>")</span><span style=color:#81a1c1>;</span></span>
<span class=giallo-l><span style=color:#8fbcbb>FlinkKafkaConsumer</span><span style=color:#eceff4>&lt;</span><span style=color:#8fbcbb>String</span><span style=color:#eceff4>></span><span> source</span><span style=color:#81a1c1> = new</span><span style=color:#8fbcbb> FlinkKafkaConsumer</span><span style=color:#eceff4>&lt;>("</span><span style=color:#a3be8c>logs.raw</span><span style=color:#eceff4>",</span><span style=color:#81a1c1> new</span><span style=color:#88c0d0> SimpleStringSchema</span><span style=color:#eceff4>(),</span><span> props</span><span style=color:#eceff4>)</span><span style=color:#81a1c1>;</span></span>
<span class=giallo-l><span style=color:#8fbcbb>DataStream</span><span style=color:#eceff4>&lt;</span><span style=color:#8fbcbb>String</span><span style=color:#eceff4>></span><span> raw</span><span style=color:#81a1c1> =</span><span> env</span><span style=color:#eceff4>.</span><span style=color:#88c0d0>addSource</span><span style=color:#eceff4>(</span><span>source</span><span style=color:#eceff4>)</span><span style=color:#81a1c1>;</span></span>
<span class=giallo-l><span style=color:#8fbcbb>DataStream</span><span style=color:#eceff4>&lt;</span><span style=color:#8fbcbb>LogEvent</span><span style=color:#eceff4>></span><span> parsed</span><span style=color:#81a1c1> =</span><span> raw</span><span style=color:#eceff4>.</span><span style=color:#88c0d0>map</span><span style=color:#eceff4>(</span><span>Json</span><span style=color:#81a1c1>::</span><span>parse</span><span style=color:#eceff4>).</span><span style=color:#88c0d0>assignTimestampsAndWatermarks</span><span style=color:#eceff4>(...)</span><span style=color:#81a1c1>;</span></span>
<span class=giallo-l><span style=color:#616e88>// 分流：检索类 → ES，分析类 → CH，明细长保留 → HBase</span></span>
<span class=giallo-l><span style=color:#8fbcbb>SideOutput</span><span> esOut</span><span style=color:#81a1c1> =</span><span style=color:#eceff4> ...</span><span style=color:#81a1c1>;</span><span style=color:#8fbcbb> SideOutput</span><span> chOut</span><span style=color:#81a1c1> =</span><span style=color:#eceff4> ...</span><span style=color:#81a1c1>;</span><span style=color:#8fbcbb> SideOutput</span><span> hbaseOut</span><span style=color:#81a1c1> =</span><span style=color:#eceff4> ...</span><span style=color:#81a1c1>;</span></span>
<span class=giallo-l><span>parsed</span><span style=color:#eceff4>.</span><span style=color:#88c0d0>process</span><span style=color:#eceff4>(</span><span style=color:#81a1c1>new</span><span style=color:#88c0d0> RouterFn</span><span style=color:#eceff4>(</span><span>esOut</span><span style=color:#eceff4>,</span><span> chOut</span><span style=color:#eceff4>,</span><span> hbaseOut</span><span style=color:#eceff4>))</span><span style=color:#81a1c1>;</span></span>
<span class=giallo-l><span style=color:#616e88>// Elasticsearch sink（bulk，低刷新）</span></span>
<span class=giallo-l><span>parsed</span><span style=color:#eceff4>.</span><span style=color:#88c0d0>getSideOutput</span><span style=color:#eceff4>(</span><span>esOut</span><span style=color:#eceff4>).</span><span style=color:#88c0d0>addSink</span><span style=color:#eceff4>(</span><span>EsSink</span><span style=color:#eceff4>.</span><span style=color:#88c0d0>bulk</span><span style=color:#eceff4>("</span><span style=color:#a3be8c>http://es:9200</span><span style=color:#eceff4>", "</span><span style=color:#a3be8c>logs-idx</span><span style=color:#eceff4>"))</span><span style=color:#81a1c1>;</span></span>
<span class=giallo-l><span style=color:#616e88>// ClickHouse sink（HTTP insert）</span></span>
<span class=giallo-l><span>parsed</span><span style=color:#eceff4>.</span><span style=color:#88c0d0>getSideOutput</span><span style=color:#eceff4>(</span><span>chOut</span><span style=color:#eceff4>).</span><span style=color:#88c0d0>addSink</span><span style=color:#eceff4>(</span><span>ClickHouseSink</span><span style=color:#eceff4>.</span><span style=color:#88c0d0>http</span><span style=color:#eceff4>("</span><span style=color:#a3be8c>http://ch:8123</span><span style=color:#eceff4>", "</span><span style=color:#a3be8c>INSERT INTO logs VALUES (?, ?, ?)</span><span style=color:#eceff4>"))</span><span style=color:#81a1c1>;</span></span>
<span class=giallo-l><span style=color:#616e88>// HBase sink（Async Put）</span></span>
<span class=giallo-l><span>parsed</span><span style=color:#eceff4>.</span><span style=color:#88c0d0>getSideOutput</span><span style=color:#eceff4>(</span><span>hbaseOut</span><span style=color:#eceff4>).</span><span style=color:#88c0d0>addSink</span><span style=color:#eceff4>(</span><span>HBaseSink</span><span style=color:#eceff4>.</span><span style=color:#88c0d0>async</span><span style=color:#eceff4>("</span><span style=color:#a3be8c>zookeeper:2181</span><span style=color:#eceff4>", "</span><span style=color:#a3be8c>logs:evt</span><span style=color:#eceff4>"))</span><span style=color:#81a1c1>;</span></span>
<span class=giallo-l><span>env</span><span style=color:#eceff4>.</span><span style=color:#88c0d0>execute</span><span style=color:#eceff4>("</span><span style=color:#a3be8c>log-pipeline</span><span style=color:#eceff4>")</span><span style=color:#81a1c1>;</span></span></code></pre><ul><li>Elasticsearch 索引与分片</ul><pre class=giallo style=color:#d8dee9;background-color:#2e3440><code data-lang=json><span class=giallo-l><span style=color:#eceff4>{</span></span>
<span class=giallo-l><span style=color:#eceff4>  "</span><span style=color:#8fbcbb>settings</span><span style=color:#eceff4>": {</span></span>
<span class=giallo-l><span style=color:#eceff4>    "</span><span style=color:#8fbcbb>number_of_shards</span><span style=color:#eceff4>":</span><span style=color:#b48ead> 3</span><span style=color:#eceff4>,</span></span>
<span class=giallo-l><span style=color:#eceff4>    "</span><span style=color:#8fbcbb>number_of_replicas</span><span style=color:#eceff4>":</span><span style=color:#b48ead> 1</span><span style=color:#eceff4>,</span></span>
<span class=giallo-l><span style=color:#eceff4>    "</span><span style=color:#8fbcbb>refresh_interval</span><span style=color:#eceff4>": "</span><span style=color:#a3be8c>5s</span><span style=color:#eceff4>",</span></span>
<span class=giallo-l><span style=color:#eceff4>    "</span><span style=color:#8fbcbb>index.translog.durability</span><span style=color:#eceff4>": "</span><span style=color:#a3be8c>async</span><span style=color:#eceff4>"</span></span>
<span class=giallo-l><span style=color:#eceff4>  },</span></span>
<span class=giallo-l><span style=color:#eceff4>  "</span><span style=color:#8fbcbb>mappings</span><span style=color:#eceff4>": {</span></span>
<span class=giallo-l><span style=color:#eceff4>    "</span><span style=color:#8fbcbb>properties</span><span style=color:#eceff4>": {</span></span>
<span class=giallo-l><span style=color:#eceff4>      "</span><span style=color:#8fbcbb>@timestamp</span><span style=color:#eceff4>": {"</span><span style=color:#8fbcbb>type</span><span style=color:#eceff4>": "</span><span style=color:#a3be8c>date</span><span style=color:#eceff4>"},</span></span>
<span class=giallo-l><span style=color:#eceff4>      "</span><span style=color:#8fbcbb>app</span><span style=color:#eceff4>": {"</span><span style=color:#8fbcbb>type</span><span style=color:#eceff4>": "</span><span style=color:#a3be8c>keyword</span><span style=color:#eceff4>"},</span></span>
<span class=giallo-l><span style=color:#eceff4>      "</span><span style=color:#8fbcbb>level</span><span style=color:#eceff4>": {"</span><span style=color:#8fbcbb>type</span><span style=color:#eceff4>": "</span><span style=color:#a3be8c>keyword</span><span style=color:#eceff4>"},</span></span>
<span class=giallo-l><span style=color:#eceff4>      "</span><span style=color:#8fbcbb>message</span><span style=color:#eceff4>": {"</span><span style=color:#8fbcbb>type</span><span style=color:#eceff4>": "</span><span style=color:#a3be8c>text</span><span style=color:#eceff4>"}</span></span>
<span class=giallo-l><span style=color:#eceff4>    }</span></span>
<span class=giallo-l><span style=color:#eceff4>  }</span></span>
<span class=giallo-l><span style=color:#eceff4>}</span></span></code></pre><ul><li>ClickHouse 表设计</ul><pre class=giallo style=color:#d8dee9;background-color:#2e3440><code data-lang=sql><span class=giallo-l><span style=color:#81a1c1>CREATE TABLE</span><span style=color:#88c0d0> logs</span><span> (</span></span>
<span class=giallo-l><span>  ts </span><span style=color:#81a1c1>DateTime</span><span>,</span></span>
<span class=giallo-l><span>  app String,</span></span>
<span class=giallo-l><span style=color:#81a1c1>  level</span><span> LowCardinality(String),</span></span>
<span class=giallo-l><span style=color:#81a1c1>  message</span><span> String</span></span>
<span class=giallo-l><span>) ENGINE </span><span style=color:#81a1c1>=</span><span> MergeTree</span></span>
<span class=giallo-l><span style=color:#81a1c1>PARTITION BY</span><span> toDate(ts)</span></span>
<span class=giallo-l><span style=color:#81a1c1>ORDER BY</span><span> (ts, app)</span></span>
<span class=giallo-l><span>SETTINGS index_granularity </span><span style=color:#81a1c1>=</span><span style=color:#b48ead> 8192</span><span>;</span></span></code></pre><ul><li>HBase RowKey 与列族</ul><pre class=giallo style=color:#d8dee9;background-color:#2e3440><code data-lang=plain><span class=giallo-l><span>RowKey: &lt;rev_ts>&lt;tenant>&lt;app>&lt;hash></span></span>
<span class=giallo-l><span>CF: d (detail), m (meta)</span></span>
<span class=giallo-l><span>预分区：按时间与租户做 split；避免热点。</span></span></code></pre><h2 id=ri-zhi-cai-ji-mo-kuai-pei-zhi-rong-qi-yu-zhu-ji>日志采集模块配置（容器与主机）</h2><h3 id=rong-qi-huan-jing-ri-zhi-cai-ji>容器环境日志采集</h3><ul><li>采集 Docker 标准输出（stdout/stderr） <ul><li>方法 A（文件采集）：Docker 默认 <code>json-file</code> 驱动将容器日志写入 <code>/var/lib/docker/containers/&lt;cid>/&lt;cid>-json.log</code>。<pre class=giallo style=color:#d8dee9;background-color:#2e3440><code data-lang=yaml><span class=giallo-l><span style=color:#8fbcbb>filebeat.inputs</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#eceff4>  -</span><span style=color:#8fbcbb> type</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> log</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    enabled</span><span style=color:#eceff4>:</span><span style=color:#81a1c1> true</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    paths</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#eceff4>      -</span><span style=color:#a3be8c> /var/lib/docker/containers/*/*-json.log</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    processors</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#eceff4>      -</span><span style=color:#8fbcbb> add_docker_metadata</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>          host</span><span style=color:#eceff4>: "</span><span style=color:#a3be8c>unix:///var/run/docker.sock</span><span style=color:#eceff4>"</span><span style=color:#616e88>  # 通过 Docker API 增强容器元数据</span></span>
<span class=giallo-l><span style=color:#eceff4>      -</span><span style=color:#8fbcbb> decode_json_fields</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>          fields</span><span style=color:#eceff4>: ["</span><span style=color:#a3be8c>message</span><span style=color:#eceff4>"]</span></span>
<span class=giallo-l><span style=color:#8fbcbb>          target</span><span style=color:#eceff4>: "</span><span style=color:#a3be8c>json</span><span style=color:#eceff4>"</span></span>
<span class=giallo-l><span style=color:#8fbcbb>          overwrite_keys</span><span style=color:#eceff4>:</span><span style=color:#81a1c1> true</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    multiline.pattern</span><span style=color:#eceff4>: '</span><span style=color:#a3be8c>^{"log":"</span><span style=color:#eceff4>'</span><span style=color:#616e88>   # 若应用以多行日志写入一条 JSON，可按需开启</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    multiline.negate</span><span style=color:#eceff4>:</span><span style=color:#81a1c1> false</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    multiline.match</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> after</span></span></code></pre><li>方法 B（容器输入）：使用 <code>type: container</code> 读取容器运行时日志（支持 Docker/Containerd CRI），适合 K8s。<pre class=giallo style=color:#d8dee9;background-color:#2e3440><code data-lang=yaml><span class=giallo-l><span style=color:#8fbcbb>filebeat.inputs</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#eceff4>  -</span><span style=color:#8fbcbb> type</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> container</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    enabled</span><span style=color:#eceff4>:</span><span style=color:#81a1c1> true</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    paths</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#eceff4>      -</span><span style=color:#a3be8c> /var/log/containers/*.log</span><span style=color:#616e88>  # Kubernetes CRI 标准路径</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    processors</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#eceff4>      -</span><span style=color:#8fbcbb> add_kubernetes_metadata</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>          host</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> ${NODE_NAME}</span></span>
<span class=giallo-l><span style=color:#8fbcbb>          matchers</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#eceff4>            -</span><span style=color:#8fbcbb> logs_path</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>                logs_path</span><span style=color:#eceff4>: "</span><span style=color:#a3be8c>/var/log/containers/</span><span style=color:#eceff4>"</span><span style=color:#616e88>  # 基于路径匹配 Pod/容器元数据</span></span>
<span class=giallo-l><span style=color:#eceff4>      -</span><span style=color:#8fbcbb> drop_event</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>          when</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>            equals</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>              kubernetes.labels.log_level</span><span style=color:#eceff4>: "</span><span style=color:#a3be8c>debug</span><span style=color:#eceff4>"</span><span style=color:#616e88>  # 示例：按标签过滤低价值日志</span></span></code></pre></ul><li>容器日志文件挂载采集 <ul><li>为 Filebeat 容器挂载主机日志目录与 Docker/K8s 元数据接口： <ul><li>Docker：挂载 <code>/var/lib/docker/containers</code> 与 <code>/var/run/docker.sock</code><li>K8s：挂载 <code>/var/log/containers</code>、<code>/var/log/pods</code>、<code>/var/lib/docker/containers</code>（取决于运行时）</ul></ul><li>Kubernetes DaemonSet 部署示例<pre class=giallo style=color:#d8dee9;background-color:#2e3440><code data-lang=yaml><span class=giallo-l><span style=color:#8fbcbb>apiVersion</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> apps/v1</span></span>
<span class=giallo-l><span style=color:#8fbcbb>kind</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> DaemonSet</span></span>
<span class=giallo-l><span style=color:#8fbcbb>metadata</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>  name</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> filebeat</span></span>
<span class=giallo-l><span style=color:#8fbcbb>  namespace</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> logging</span></span>
<span class=giallo-l><span style=color:#8fbcbb>spec</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>  selector</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    matchLabels</span><span style=color:#eceff4>: {</span><span style=color:#8fbcbb>app</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> filebeat</span><span style=color:#eceff4>}</span></span>
<span class=giallo-l><span style=color:#8fbcbb>  template</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    metadata</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>      labels</span><span style=color:#eceff4>: {</span><span style=color:#8fbcbb>app</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> filebeat</span><span style=color:#eceff4>}</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    spec</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>      serviceAccountName</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> filebeat</span></span>
<span class=giallo-l><span style=color:#8fbcbb>      hostNetwork</span><span style=color:#eceff4>:</span><span style=color:#81a1c1> true</span></span>
<span class=giallo-l><span style=color:#8fbcbb>      dnsPolicy</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> ClusterFirstWithHostNet</span></span>
<span class=giallo-l><span style=color:#8fbcbb>      containers</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#eceff4>        -</span><span style=color:#8fbcbb> name</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> filebeat</span></span>
<span class=giallo-l><span style=color:#8fbcbb>          image</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> docker.elastic.co/beats/filebeat:8.12.0</span></span>
<span class=giallo-l><span style=color:#8fbcbb>          args</span><span style=color:#eceff4>: ["</span><span style=color:#a3be8c>-c</span><span style=color:#eceff4>", "</span><span style=color:#a3be8c>/etc/filebeat.yml</span><span style=color:#eceff4>", "</span><span style=color:#a3be8c>-e</span><span style=color:#eceff4>"]</span></span>
<span class=giallo-l><span style=color:#8fbcbb>          env</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#eceff4>            -</span><span style=color:#8fbcbb> name</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> NODE_NAME</span></span>
<span class=giallo-l><span style=color:#8fbcbb>              valueFrom</span><span style=color:#eceff4>: {</span><span style=color:#8fbcbb>fieldRef</span><span style=color:#eceff4>: {</span><span style=color:#8fbcbb>fieldPath</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> spec.nodeName</span><span style=color:#eceff4>}}</span></span>
<span class=giallo-l><span style=color:#8fbcbb>          volumeMounts</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#eceff4>            - {</span><span style=color:#8fbcbb>name</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> config</span><span style=color:#eceff4>,</span><span style=color:#8fbcbb> mountPath</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> /etc/filebeat.yml</span><span style=color:#eceff4>,</span><span style=color:#8fbcbb> subPath</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> filebeat.yml</span><span style=color:#eceff4>}</span></span>
<span class=giallo-l><span style=color:#eceff4>            - {</span><span style=color:#8fbcbb>name</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> varlog</span><span style=color:#eceff4>,</span><span style=color:#8fbcbb> mountPath</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> /var/log</span><span style=color:#eceff4>}</span></span>
<span class=giallo-l><span style=color:#eceff4>            - {</span><span style=color:#8fbcbb>name</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> containers</span><span style=color:#eceff4>,</span><span style=color:#8fbcbb> mountPath</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> /var/lib/docker/containers</span><span style=color:#eceff4>,</span><span style=color:#8fbcbb> readOnly</span><span style=color:#eceff4>:</span><span style=color:#81a1c1> true</span><span style=color:#eceff4>}</span></span>
<span class=giallo-l><span style=color:#eceff4>            - {</span><span style=color:#8fbcbb>name</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> dockersock</span><span style=color:#eceff4>,</span><span style=color:#8fbcbb> mountPath</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> /var/run/docker.sock</span><span style=color:#eceff4>}</span></span>
<span class=giallo-l><span style=color:#8fbcbb>          securityContext</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>            runAsUser</span><span style=color:#eceff4>:</span><span style=color:#b48ead> 0</span></span>
<span class=giallo-l><span style=color:#8fbcbb>      volumes</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#eceff4>        -</span><span style=color:#8fbcbb> name</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> config</span></span>
<span class=giallo-l><span style=color:#8fbcbb>          configMap</span><span style=color:#eceff4>: {</span><span style=color:#8fbcbb>name</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> filebeat-config</span><span style=color:#eceff4>}</span></span>
<span class=giallo-l><span style=color:#eceff4>        -</span><span style=color:#8fbcbb> name</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> varlog</span></span>
<span class=giallo-l><span style=color:#8fbcbb>          hostPath</span><span style=color:#eceff4>: {</span><span style=color:#8fbcbb>path</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> /var/log</span><span style=color:#eceff4>}</span></span>
<span class=giallo-l><span style=color:#eceff4>        -</span><span style=color:#8fbcbb> name</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> containers</span></span>
<span class=giallo-l><span style=color:#8fbcbb>          hostPath</span><span style=color:#eceff4>: {</span><span style=color:#8fbcbb>path</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> /var/lib/docker/containers</span><span style=color:#eceff4>}</span></span>
<span class=giallo-l><span style=color:#eceff4>        -</span><span style=color:#8fbcbb> name</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> dockersock</span></span>
<span class=giallo-l><span style=color:#8fbcbb>          hostPath</span><span style=color:#eceff4>: {</span><span style=color:#8fbcbb>path</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> /var/run/docker.sock</span><span style=color:#eceff4>}</span></span>
<span class=giallo-l><span>---</span></span>
<span class=giallo-l><span style=color:#8fbcbb>apiVersion</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> v1</span></span>
<span class=giallo-l><span style=color:#8fbcbb>kind</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> ConfigMap</span></span>
<span class=giallo-l><span style=color:#8fbcbb>metadata</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>  name</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> filebeat-config</span></span>
<span class=giallo-l><span style=color:#8fbcbb>  namespace</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> logging</span></span>
<span class=giallo-l><span style=color:#8fbcbb>data</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>  filebeat.yml</span><span style=color:#eceff4>:</span><span style=color:#81a1c1> |</span></span>
<span class=giallo-l><span style=color:#a3be8c>    filebeat.inputs:</span></span>
<span class=giallo-l><span style=color:#a3be8c>      - type: container</span></span>
<span class=giallo-l><span style=color:#a3be8c>        enabled: true</span></span>
<span class=giallo-l><span style=color:#a3be8c>        paths:</span></span>
<span class=giallo-l><span style=color:#a3be8c>          - /var/log/containers/*.log</span></span>
<span class=giallo-l><span style=color:#a3be8c>        processors:</span></span>
<span class=giallo-l><span style=color:#a3be8c>          - add_kubernetes_metadata:</span></span>
<span class=giallo-l><span style=color:#a3be8c>              host: ${NODE_NAME}</span></span>
<span class=giallo-l><span style=color:#a3be8c>              matchers:</span></span>
<span class=giallo-l><span style=color:#a3be8c>                - logs_path:</span></span>
<span class=giallo-l><span style=color:#a3be8c>                    logs_path: "/var/log/containers/"</span></span>
<span class=giallo-l><span style=color:#a3be8c>    output.kafka:</span></span>
<span class=giallo-l><span style=color:#a3be8c>      hosts: ["kafka1:9092","kafka2:9092"]</span></span>
<span class=giallo-l><span style=color:#a3be8c>      topic: "logs.raw"</span></span>
<span class=giallo-l><span style=color:#a3be8c>      partition.round_robin.reachable_only: true</span></span>
<span class=giallo-l><span style=color:#a3be8c>      compression: lz4</span></span>
<span class=giallo-l><span style=color:#a3be8c>      required_acks: -1</span></span></code></pre><li>容器元数据增强 <ul><li>Docker：<code>add_docker_metadata</code> 通过 Docker API 提取容器名、镜像、标签等；<li>Kubernetes：<code>add_kubernetes_metadata</code> 注入 Pod/Namespace/Labels/Annotations；可用 <code>drop_event</code>/<code>drop_fields</code> 做过滤与瘦身。</ul></ul><h3 id=zhu-ji-huan-jing-ri-zhi-cai-ji>主机环境日志采集</h3><ul><li>采集系统日志（syslog/auth/journald） <ul><li>使用系统模块（推荐）：<pre class=giallo style=color:#d8dee9;background-color:#2e3440><code data-lang=yaml><span class=giallo-l><span style=color:#8fbcbb>filebeat.modules</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#eceff4>  -</span><span style=color:#8fbcbb> module</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> system</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    syslog</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>      enabled</span><span style=color:#eceff4>:</span><span style=color:#81a1c1> true</span></span>
<span class=giallo-l><span style=color:#8fbcbb>      var.paths</span><span style=color:#eceff4>: ["</span><span style=color:#a3be8c>/var/log/syslog</span><span style=color:#eceff4>", "</span><span style=color:#a3be8c>/var/log/messages</span><span style=color:#eceff4>"]</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    auth</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>      enabled</span><span style=color:#eceff4>:</span><span style=color:#81a1c1> true</span></span>
<span class=giallo-l><span style=color:#8fbcbb>      var.paths</span><span style=color:#eceff4>: ["</span><span style=color:#a3be8c>/var/log/auth.log</span><span style=color:#eceff4>", "</span><span style=color:#a3be8c>/var/log/secure</span><span style=color:#eceff4>"]</span></span></code></pre><li>或文件输入（通用）：<pre class=giallo style=color:#d8dee9;background-color:#2e3440><code data-lang=yaml><span class=giallo-l><span style=color:#8fbcbb>filebeat.inputs</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#eceff4>  -</span><span style=color:#8fbcbb> type</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> log</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    paths</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#eceff4>      -</span><span style=color:#a3be8c> /var/log/*.log</span></span>
<span class=giallo-l><span style=color:#eceff4>      -</span><span style=color:#a3be8c> /var/log/nginx/*.log</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    multiline.pattern</span><span style=color:#eceff4>: '</span><span style=color:#a3be8c>^\\[</span><span style=color:#eceff4>'</span><span style=color:#616e88>  # 以时间/方括号开始的堆栈等多行</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    multiline.negate</span><span style=color:#eceff4>:</span><span style=color:#81a1c1> true</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    multiline.match</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> after</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    fields_under_root</span><span style=color:#eceff4>:</span><span style=color:#81a1c1> true</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    fields</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>      host.role</span><span style=color:#eceff4>: "</span><span style=color:#a3be8c>web</span><span style=color:#eceff4>"</span></span>
<span class=giallo-l><span style=color:#8fbcbb>      env</span><span style=color:#eceff4>: "</span><span style=color:#a3be8c>prod</span><span style=color:#eceff4>"</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    processors</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#eceff4>      -</span><span style=color:#8fbcbb> drop_fields</span><span style=color:#eceff4>: {</span><span style=color:#8fbcbb>fields</span><span style=color:#eceff4>: ["</span><span style=color:#a3be8c>log.offset</span><span style=color:#eceff4>", "</span><span style=color:#a3be8c>input.type</span><span style=color:#eceff4>"]}</span></span>
<span class=giallo-l><span style=color:#eceff4>      -</span><span style=color:#8fbcbb> rename</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>          fields</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#eceff4>            -</span><span style=color:#8fbcbb> from</span><span style=color:#eceff4>: "</span><span style=color:#a3be8c>host.name</span><span style=color:#eceff4>"</span></span>
<span class=giallo-l><span style=color:#8fbcbb>              to</span><span style=color:#eceff4>: "</span><span style=color:#a3be8c>hostname</span><span style=color:#eceff4>"</span></span></code></pre></ul><li>主机标签与自定义字段 <ul><li>使用 <code>fields</code> 与 <code>fields_under_root: true</code> 直接展开到事件根，提高检索友好性；配合 <code>tags: [prod, region-cn]</code> 标注环境。</ul><li>日志轮转与归档建议 <ul><li>应用侧启用 <code>logrotate</code> 或等效机制，控制单文件大小与保留周期；<li>Filebeat 相关参数： <ul><li><code>close_inactive: 5m</code>（无新数据关闭文件句柄）<li><code>clean_inactive: 168h</code>（长时间无活动的文件从注册表清理）<li><code>ignore_older: 24h</code>（忽略过久文件）<li><code>scan_frequency: 10s</code>（扫描新文件频率）</ul></ul></ul><h3 id=tong-yong-pei-zhi-yao-qiu>通用配置要求</h3><ul><li>完整的 <code>filebeat.yml</code> 示例（容器与主机通用骨架）</ul><pre class=giallo style=color:#d8dee9;background-color:#2e3440><code data-lang=yaml><span class=giallo-l><span style=color:#8fbcbb>filebeat.inputs</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#eceff4>  -</span><span style=color:#8fbcbb> type</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> log</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    enabled</span><span style=color:#eceff4>:</span><span style=color:#81a1c1> true</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    paths</span><span style=color:#eceff4>: ["</span><span style=color:#a3be8c>/var/log/app/*.log</span><span style=color:#eceff4>"]</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    multiline.pattern</span><span style=color:#eceff4>: '</span><span style=color:#a3be8c>^\\[</span><span style=color:#eceff4>'</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    multiline.negate</span><span style=color:#eceff4>:</span><span style=color:#81a1c1> true</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    multiline.match</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> after</span></span>
<span class=giallo-l><span style=color:#8fbcbb>    processors</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#eceff4>      -</span><span style=color:#8fbcbb> drop_event</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>          when</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>            equals</span><span style=color:#eceff4>: {</span><span style=color:#8fbcbb>log.level</span><span style=color:#eceff4>: "</span><span style=color:#a3be8c>debug</span><span style=color:#eceff4>"}</span></span>
<span class=giallo-l><span style=color:#eceff4>      -</span><span style=color:#8fbcbb> decode_json_fields</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>          fields</span><span style=color:#eceff4>: ["</span><span style=color:#a3be8c>message</span><span style=color:#eceff4>"]</span></span>
<span class=giallo-l><span style=color:#8fbcbb>          target</span><span style=color:#eceff4>: "</span><span style=color:#a3be8c>json</span><span style=color:#eceff4>"</span></span>
<span class=giallo-l><span style=color:#8fbcbb>          overwrite_keys</span><span style=color:#eceff4>:</span><span style=color:#81a1c1> true</span></span>
<span class=giallo-l><span style=color:#eceff4>      -</span><span style=color:#8fbcbb> add_docker_metadata</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>          host</span><span style=color:#eceff4>: "</span><span style=color:#a3be8c>unix:///var/run/docker.sock</span><span style=color:#eceff4>"</span></span>
<span class=giallo-l><span style=color:#eceff4>      -</span><span style=color:#8fbcbb> add_kubernetes_metadata</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>          host</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> ${NODE_NAME}</span></span>
<span class=giallo-l><span style=color:#8fbcbb>output.kafka</span><span style=color:#eceff4>:</span></span>
<span class=giallo-l><span style=color:#8fbcbb>  hosts</span><span style=color:#eceff4>: ["</span><span style=color:#a3be8c>kafka1:9092</span><span style=color:#eceff4>","</span><span style=color:#a3be8c>kafka2:9092</span><span style=color:#eceff4>"]</span></span>
<span class=giallo-l><span style=color:#8fbcbb>  topic</span><span style=color:#eceff4>: "</span><span style=color:#a3be8c>logs.raw</span><span style=color:#eceff4>"</span></span>
<span class=giallo-l><span style=color:#8fbcbb>  compression</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> lz4</span></span>
<span class=giallo-l><span style=color:#8fbcbb>  partition.round_robin.reachable_only</span><span style=color:#eceff4>:</span><span style=color:#81a1c1> true</span></span>
<span class=giallo-l><span style=color:#8fbcbb>  required_acks</span><span style=color:#eceff4>:</span><span style=color:#b48ead> -1</span></span>
<span class=giallo-l><span style=color:#8fbcbb>  client_id</span><span style=color:#eceff4>: "</span><span style=color:#a3be8c>filebeat-prod</span><span style=color:#eceff4>"</span></span>
<span class=giallo-l><span style=color:#8fbcbb>  max_message_bytes</span><span style=color:#eceff4>:</span><span style=color:#b48ead> 1000000</span></span>
<span class=giallo-l><span style=color:#616e88># 安全（示例，按需启用）</span></span>
<span class=giallo-l><span style=color:#616e88># ssl.certificate_authorities: ["/etc/ssl/ca.pem"]</span></span>
<span class=giallo-l><span style=color:#616e88># sasl.mechanism: scram-sha-512</span></span>
<span class=giallo-l><span style=color:#616e88># username: "beat_user"</span></span>
<span class=giallo-l><span style=color:#616e88># password: "secret"</span></span>
<span class=giallo-l><span style=color:#616e88># 监控接口</span></span>
<span class=giallo-l><span style=color:#8fbcbb>http.enabled</span><span style=color:#eceff4>:</span><span style=color:#81a1c1> true</span></span>
<span class=giallo-l><span style=color:#8fbcbb>http.host</span><span style=color:#eceff4>:</span><span style=color:#b48ead> 0.0.0.0</span></span>
<span class=giallo-l><span style=color:#8fbcbb>http.port</span><span style=color:#eceff4>:</span><span style=color:#b48ead> 5066</span></span>
<span class=giallo-l><span style=color:#8fbcbb>logging.level</span><span style=color:#eceff4>:</span><span style=color:#a3be8c> info</span></span>
<span class=giallo-l><span style=color:#8fbcbb>logging.selectors</span><span style=color:#eceff4>: ["</span><span style=color:#a3be8c>publish</span><span style=color:#eceff4>", "</span><span style=color:#a3be8c>processors</span><span style=color:#eceff4>"]</span></span></code></pre><ul><li>过滤与字段处理 <ul><li>首选在采集侧做降噪：<code>drop_event</code>、<code>drop_fields</code>、<code>dissect</code>/<code>decode_json_fields</code>、<code>rename</code>；<li>统一时间戳：<code>timestamp</code> 处理器将自定义时间字段映射到 <code>@timestamp</code>。</ul><li>Kafka 输出参数说明 <ul><li><code>required_acks=-1</code>（all ISR 确认）、<code>compression=lz4</code>、<code>max_message_bytes</code> 控制单消息大小；<li><code>partition.round_robin.reachable_only=true</code> 避免不可达分区；<code>client_id</code> 标识采集实例；<li>安全：SASL SCRAM 与 SSL 证书链；生产环境建议启用加密与认证。</ul><li>性能与安全建议 <ul><li>控制输入文件数量与 <code>harvester_limit</code>，避免句柄耗尽；<li>合理 <code>bulk_max_size</code> 与 Kafka 端 <code>linger.ms</code>/<code>batch.size</code> 匹配；<li>采集侧最小权限运行，限制可读取路径；容器中避免特权运行。</ul></ul><h3 id=yan-zheng-fang-an>验证方案</h3><ul><li>功能验证 <ul><li>本地：<code>filebeat test output</code> 检查与 Kafka 的连通；<code>filebeat -e -d "publish"</code> 观察事件发送；<li>端到端：使用 <code>kcat -C -b kafka1:9092 -t logs.raw -o -10</code> 拉取最新评论，校验字段；</ul><li>常见问题排查 <ul><li>容器元数据缺失：检查是否挂载 <code>/var/run/docker.sock</code> 或正确配置 <code>add_kubernetes_metadata</code>；<li>多行合并异常：确认 <code>multiline.pattern</code> 与日志格式匹配，适当调整 <code>timeout</code>；<li>无法读取文件：校验路径与权限，确认 Filebeat 运行用户与宿主机挂载；<li>Kafka 429/缓慢：检查 <code>required_acks</code>、网络与批量参数，监控 broker 队列与磁盘；</ul><li>性能监控配置建议 <ul><li>启用 <code>http.enabled: true</code> 暴露指标（<code>/stats</code>），采集 <code>beat.events</code>、<code>harvester.open_files</code>、<code>output.events</code> 成功/失败；<li>用 Prometheus 抓取并设置告警：采集速率低于预期、发送失败率升高、打开文件数异常增长。</ul></ul><h1 id=4-xing-neng-you-hua-jing-yan>4. 性能优化经验</h1><ul><li>Filebeat：<code>bulk_max_size</code> 调整批量；启用持久队列；过滤低价值日志降低链路压力。<li>Kafka：合适分区数与 RF；Broker <code>num.network.threads</code>/<code>socket.send.buffer.bytes</code>；启用 LZ4；控制批大小与 linger。<li>Flink：优化并行度与算子链；RocksDB state、增大 <code>write-buffer-size</code>；Checkpoint 间隔与超时合理化。<li>ES：降低刷新频率（5-10s）、增大 <code>indexing.buffer</code>、合理分片；使用 ILM 滚动与冷/温节点。<li>CH：控制 parts 数量，批量写入；<code>max_insert_block_size</code>、<code>max_threads</code> 调优；后台合并监控。<li>HBase：RowKey 防热点；<code>memstore</code>/<code>blockcache</code> 调整；Major/Minor compaction 节奏。<li>容量规划：按峰值吞吐与增长率规划分区/分片/Region 数量，留有 30% 余量。</ul><h1 id=5-yun-wei-jian-kong-fang-an>5. 运维监控方案</h1><ul><li>指标 <ul><li>Filebeat：harvesters、publish queue 使用率、掉包率。<li>Kafka：ISR、Under-Replicated Partitions、Consumer Lag、请求队列长度。<li>Flink：Backpressure、Checkpoint Time/Fail、Task Failures。<li>ES：JVM heap、GC、Indexing rate、Query p95/p99、Threadpool 队列。<li>CH：parts 数、后台 merges、replication lag、查询耗时分布。<li>HBase：RegionServer 负载、Compaction、读写延迟。</ul><li>告警阈值示例（PromQL）</ul><pre class=giallo style=color:#d8dee9;background-color:#2e3440><code data-lang=plain><span class=giallo-l><span>kafka_server_replica_manager_underreplicatedpartitions > 0</span></span>
<span class=giallo-l><span>sum(rate(es_indexing_index_total[5m])) &lt; expected_rate * 0.7</span></span></code></pre><ul><li>运维清单：滚动升级、备份与快照、容量巡检、索引与表维护、Checkpoint 与 Savepoint 验证。</ul><h1 id=6-shi-ji-ying-yong-an-li-shi-li>6. 实际应用案例（示例）</h1><ul><li>规模：日均 2TB，峰值 250k events/s；端到端 p95 ≤ 500ms。<li>查询：ES 关键词检索 p95 ≤ 200ms；CH 1 亿行聚合 p95 ≤ 2s。<li>价值：统一日志入口、降低故障定位时间 ≥50%，支持审计与增长分析。</ul><h1 id=7-bu-shu-jiao-ben-shi-li-pian-duan>7. 部署脚本示例（片段）</h1><pre class=giallo style=color:#d8dee9;background-color:#2e3440><code data-lang=shellscript><span class=giallo-l><span style=color:#616e88>#!/usr/bin/env bash</span></span>
<span class=giallo-l><span style=color:#88c0d0>set</span><span style=color:#a3be8c> -euo pipefail</span></span>
<span class=giallo-l><span style=color:#616e88># 安装并启动 Filebeat（示例）</span></span>
<span class=giallo-l><span style=color:#88c0d0>apt-get</span><span style=color:#a3be8c> update</span><span style=color:#eceff4> &&</span><span style=color:#88c0d0> apt-get</span><span style=color:#a3be8c> install -y filebeat</span></span>
<span class=giallo-l><span style=color:#88c0d0>cp</span><span style=color:#a3be8c> filebeat.yml /etc/filebeat/filebeat.yml</span></span>
<span class=giallo-l><span style=color:#88c0d0>systemctl</span><span style=color:#a3be8c> enable filebeat</span><span style=color:#eceff4> &&</span><span style=color:#88c0d0> systemctl</span><span style=color:#a3be8c> restart filebeat</span></span>
<span class=giallo-l><span style=color:#616e88># Kafka Producer 依赖 JDK</span></span>
<span class=giallo-l><span style=color:#81a1c1>export</span><span> JAVA_HOME</span><span style=color:#81a1c1>=</span><span>/usr/lib/jvm/java-17-openjdk-amd64</span></span>
<span class=giallo-l><span style=color:#616e88># Flink 提交示例</span></span>
<span class=giallo-l><span style=color:#88c0d0>flink</span><span style=color:#a3be8c> run -c com.example.LogPipeline ./log-pipeline.jar</span></span></code></pre><h1 id=8-dian-xing-wen-ti-yu-jie-jue>8. 典型问题与解决</h1><ul><li>Kafka 负载不均：分区键散列不均导致热点 → 优化 Key 与增加分区。<li>Flink Checkpoint 失败：外部存储带宽不足 → 升级存储与拉长间隔、压缩状态。<li>ES 写入 429：线程池队列满 → 降低刷新频率、调大 bulk、限流入口。<li>CH parts 过多：小批次写导致合并压力 → 增大批次与控制并发。<li>HBase 热点 Region：RowKey 顺序导致集中 → 前缀散列或反转时间戳。</ul><h1 id=9-dui-bi-chuan-tong-fang-an-de-gai-jin>9. 对比传统方案的改进</h1><ul><li>相比仅 ELK：引入 Kafka 与 Flink 实现解耦与实时计算；ClickHouse/HBase 提供高效分析与长期明细，整体可用性与扩展性更佳。</ul><h1 id=10-jia-gou-yan-jin-fang-xiang>10. 架构演进方向</h1><ul><li>Schema Registry+Avro/Protobuf；Kafka 分层存储；Flink Stateful Functions；ES 冷/温节点与 CCR；ClickHouse 对象存储分层；HBase 与 Data Lake（Iceberg/Hudi）打通。</ul><h1 id=hou-tai-kong-zhi-xi-tong-xiang-xi-she-ji>后台控制系统详细设计</h1><p>为支撑日志平台的可配置性、查询可扩展性与稳定告警能力，设计统一的后台控制系统，包含采集配置模块、查询分析模块与日志告警模块。每个模块提供架构图、技术选型、性能指标、接口定义与异常处理及一致性保障方案。<h2 id=1-cai-ji-pei-zhi-mo-kuai>1) 采集配置模块</h2><ul><li><p>架构设计图： <img alt=采集配置中心架构 src=/backend-control-config-architecture.svg></p><li><p>关键技术选型说明</p> <ul><li>配置中心：<code>PostgreSQL</code> 存储配置与审计，<code>Git</code> 作为版本库（拉取与回滚），<code>etcd/Consul</code> 用于轻量型在线 KV 与 Watch；通过 <code>OpenAPI</code> 提供读写接口。<li>变更分发：<code>Kafka</code> 主题 <code>config-updates</code> 广播配置变更；<code>Flink</code>/微服务消费后生成增量快照。<li>Agent 管理：Filebeat、Fluent Bit、自研采集代理统一通过 <code>HTTPS/gRPC</code> 拉取配置或被动订阅；支持多协议采集（HTTP/HTTPS/TCP/UDP）。<li>校验与灰度：配置 Schema（<code>JSON Schema</code>/<code>Protobuf</code>）校验，支持批次灰度与逐步扩散；失败自动回滚。</ul><li><p>配置项设计</p> <ul><li>采集频率：<code>scan_frequency</code>、<code>harvester_limit</code>、<code>ignore_older/close_inactive</code>；<li>数据格式：<code>json/line/custom</code>，字段映射与 <code>decode_json_fields/dissect</code>；<li>过滤规则：<code>drop_event/drop_fields/rename/timestamp</code>；<li>协议参数：HTTP/HTTPS（<code>headers/auth/timeout</code>）、TCP/UDP（<code>host/port/max_message_bytes</code>）。<li>版本控制：每次提交生成 <code>versionId</code>（Git commit），支持标签与回滚；所有变更留审计轨迹。</ul><li><p>性能指标要求</p> <ul><li>吞吐：配置读取 <code>p99 ≥ 2k rps</code>；写入 <code>p99 ≥ 200 rps</code>。<li>传播延迟：集群内 <code>p95 ≤ 5s</code> 到达所有在线 Agent。<li>可用性：<code>≥ 99.95%</code>（双活/主备部署）。</ul><li><p>与其他模块交互接口定义</p> <ul><li>REST/gRPC（示例）： <ul><li><code>POST /api/v1/configs</code>（创建配置，返回 <code>versionId</code>）<li><code>GET /api/v1/configs/{id}</code>（读取配置）<li><code>POST /api/v1/configs/{id}/deploy?strategy=canary</code>（灰度发布）<li><code>POST /api/v1/configs/{id}/rollback</code>（版本回滚）</ul><li>Kafka：<code>topic=config-updates</code>（key: <code>agentId/group</code>，value: <code>configVersion</code>）。</ul><li><p>异常处理与一致性保障</p> <ul><li>乐观并发控制（<code>version</code> 字段）；写前校验与写后审计。<li>分发失败重试与死信队列（DLQ）；Agent 端本地快照与超时回退策略。<li>事件顺序保障：<code>configVersion</code> 单调递增，Agent 只接受更高版本；幂等应用。<li>部署回滚与熔断：批次灰度监控异常触发自动回滚；对下游 Kafka/HTTP 超时启用熔断。</ul></ul><h2 id=2-cha-xun-fen-xi-mo-kuai>2) 查询分析模块</h2><ul><li><p>架构设计图： <img alt=查询分析引擎架构 src=/backend-control-query-architecture.svg></p><li><p>关键技术选型说明</p> <ul><li>分布式查询引擎：<code>Trino/Presto</code> 用于 SQL 跨源查询（ES/ClickHouse/HBase 适配器）；<li>DSL 引擎：<code>Lucene/KQL</code> 风格 DSL 转换为 ES 查询；复杂聚合落 CH 物化视图。<li>缓存与索引优化：<code>Redis</code> 查询结果缓存（Key=归一化查询+时间窗）；CH 物化视图与稀疏索引；ES 使用 <code>keyword</code> 与 <code>nested</code> 正确映射。<li>路由层：根据查询类型与代价估算路由至 ES（检索）或 CH（聚合），支持双写兜底。</ul><li><p>多维聚合功能</p> <ul><li>维度：<code>app/env/host/region/userId</code> 等，支持 <code>group by/rollup/cube</code>；<li>时间窗：<code>HOPPING/TUMBLING/SLIDING</code>；预聚合表加速报表查询。</ul><li><p>性能指标要求</p> <ul><li>吞吐：并发查询 <code>p99 ≥ 500 qps</code>（缓存命中场景）；<li>延迟：实时检索 <code>p95 ≤ 300ms</code>（ES）；重聚合 <code>p95 ≤ 2s</code>（CH）；<li>可用性：<code>≥ 99.9%</code>，路由层与引擎多副本。</ul><li><p>与其他模块交互接口定义</p> <ul><li>HTTP/WS： <ul><li><code>POST /api/v1/query/sql</code>（body: SQL，支持分页与超时）<li><code>POST /api/v1/query/dsl</code>（body: DSL，支持流式返回）<li><code>GET /api/v1/query/{id}/status</code>（异步查询状态）</ul><li>结果缓存：<code>GET /api/v1/query/cache/{hash}</code>；<code>DELETE /api/v1/query/cache/{hash}</code>。</ul><li><p>异常处理与一致性保障</p> <ul><li>超时与降级：超过 SLA 自动降级为预聚合与近似结果；<li>重试与幂等：幂等查询 ID；后端重试限制与指数退避；<li>数据一致性：跨源时间窗对齐；ES/CH 双写校验差异并标记结果版本；<li>背压控制：路由层限流与队列；防止查询风暴。</ul></ul><h2 id=3-ri-zhi-gao-jing-mo-kuai>3) 日志告警模块</h2><ul><li><p>架构设计图： <img alt=日志告警引擎架构 src=/backend-control-alert-architecture.svg></p><li><p>关键技术选型说明</p> <ul><li>规则引擎：<code>Flink</code> 流式评估（事件时间），支持多级告警（INFO/WARNING/ERROR/CRITICAL）；<li>时间窗口：<code>Sliding/Tumbling</code> 窗口，支持去重键与聚合阈值；<li>通知通道：<code>SMTP</code> 邮件、短信网关（<code>HTTP</code> SDK）、<code>Webhook</code>（签名校验与重试）；<li>抑制与降噪：重复告警抑制、合并策略、秒级阈值与指数退避；支持维护窗口与静默策略。</ul><li><p>性能指标要求</p> <ul><li>吞吐：规则评估 <code>≥ 1M events/min</code>；<li>延迟：端到端告警触发 <code>p95 ≤ 5s</code>；<li>可用性：<code>≥ 99.9%</code>，检查点与状态容灾。</ul><li><p>与其他模块交互接口定义</p> <ul><li>规则管理： <ul><li><code>POST /api/v1/alerts/rules</code>（创建/更新，返回 <code>ruleId/version</code>）<li><code>GET /api/v1/alerts/rules/{id}</code>（读取）<li><code>POST /api/v1/alerts/rules/{id}/disable</code>（禁用）</ul><li>告警事件： <ul><li><code>GET /api/v1/alerts/events?level=ERROR&window=1h</code>（查询告警事件）<li><code>POST /api/v1/alerts/notifications/test</code>（通道联通性测试）</ul><li>Kafka：<code>topic=alerts-input</code>（原始事件），<code>topic=alerts-output</code>（告警结果）。</ul><li><p>异常处理与一致性保障</p> <ul><li>Exactly-Once：Kafka + Flink 两阶段提交，通知端幂等；<li>顺序与乱序：事件时间窗口处理，迟到数据允许度 <code>allowedLateness</code>；<li>抑制策略：重复告警聚合与限流，按 <code>dedupKey</code> 与时间窗去重；<li>故障自愈：checkpoint 失败报警与自动重启；通道失败重试与 DLQ。</ul></ul></div></div></section>